[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modelling Spatiotemporal Processes",
    "section": "",
    "text": "Course overview"
  },
  {
    "objectID": "index.html#literature",
    "href": "index.html#literature",
    "title": "Modelling Spatiotemporal Processes",
    "section": "Literature",
    "text": "Literature\n\nC. Chatfield, The analysis of time series: an introduction. Chapman and Hall: chapters 1, 2 and 3 (Chatfield 2003)\nSpatial Data Science, with applications in R (Pebesma and Bivand 2022):\n\nCh 1 (intro), 7 (sf, stars)\nCh 12 (interpolation)"
  },
  {
    "objectID": "index.html#organization",
    "href": "index.html#organization",
    "title": "Modelling Spatiotemporal Processes",
    "section": "Organization",
    "text": "Organization\nTeachers:\n\nChristian Knoth (exercises, Wed 12-14)\nEdzer Pebesma (lectures)\n\nLearnweb:\n\nsubscribe\nno password\nLectures + exercises is only one course.\n\nSlides:\n\nrendered: https://edzer.github.io/mstp/\nqmd (quarto) sources on on http://github.com/edzer/mstp\nyou can load and run the individual qmd files in rstudio\npull requests with improvements are appreciated (and may be rewarded):\n\nfork the repository on GitHub,\nclick “Edit this page” on the right-hand-side\n\n\n\nExamen:\n\nmultiple choice, 4 possibilities, 40 questions, 20 need to be correct.\n\n\n\nOverview of the course\nTopics:\n\nTime series data\nTime series models: AR(p), MA(q), partial correlation, AIC, forecasting\nOptimisation:\n\nLinear models, least squares: normal equations\nNon-linear:\n\nOne-dimensional: golden search\nMulti-dimensional least squares: Newton\nMulti-dimensional stochastic search: Metropolis\nMulti-dimensional stochastic optimisation: Metropolis\n\n\nSpatial models:\n\nSimple, heuristic spatial interpolation approaches\nSpatial correlation\nRegression with spatially correlated data\nKriging: best linear (unbiased) prediction\nStationarity, variogram\nKriging varieties: simple, ordinary, universal kriging\nKriging as a probabilistic spatial predictor\n\nSpatio-temporal variation modelled by partial differential equations\n\nInitial and boundary conditions\nExample\nCalibration: Kalman filter"
  },
  {
    "objectID": "index.html#where-we-come-from",
    "href": "index.html#where-we-come-from",
    "title": "Modelling Spatiotemporal Processes",
    "section": "Where we come from",
    "text": "Where we come from\n\nintroduction to geostatistics\nmathematics, linear algebra\ncomputer science\n\n\nIntroduction to geostatistics\n\ntypes of variables: Stevens’ measurement scales – nominal, ordinal, interval, ratio\n… or: discrete, continuous\nt-tests, ANOVA\nregression, multiple regression (but not how we compute it)\nassumption was: observations are independent\nwhat does independence mean?\n\n\n\nIn this course\n\nwe will study dependence in observations, in\n\nspace\ntime\nor space-time\n\nin space and/or time, Stevens’ measurement scales are not enough! Examples:\n\nlinear time, cyclic time\nspace: functions, fields\n\nwe will study how we can represent phenomena, by\n\nmathematical representations (models)\ncomputer representations (models)\n\nwe will consider how well these models correspond to our observations"
  },
  {
    "objectID": "index.html#spatio-temporal-phenomena-are-everywhere",
    "href": "index.html#spatio-temporal-phenomena-are-everywhere",
    "title": "Modelling Spatiotemporal Processes",
    "section": "Spatio-temporal phenomena are everywhere",
    "text": "Spatio-temporal phenomena are everywhere\n\nif we think about it, there are no data that can be non-spatial or non-temporal.\nin many cases, the spatial or temporal references are not essential\n\nthink: brain image of a person: time matters, but mostly referenced with respect to the age of the person, spatial location of the MRI scanner does not\nbut: ID of the patient does!\nand: time of scan matters too!\n\nwe will “pigeon-hole” (classify) phenomena into: fields, objects, aggregations\n\n\nFields\n\nmany processes can be represented by fields, meaning they could be measured everywhere\nthink: temperature in this room\ntypical problems: interpolation, patterns, trends, temporal development, forecasting?\n\n\n\nObjects and events\n\nobjects can be identified\nobjects are identified within a frame (or window) of observation\nwithin this window, between objects, there are no objects (no point of interpolation)\nobjects can be moving (people), or static (buildings)\nobjects or events are sometimes obtained by thresholding fields, think heat wave, earthquake, hurricane, (see e.g. Camara et al. 2014)\nsometimes this view is rather artificial, think cars, persons, buildings\n\n\n\nFields - objects/events conversions\n\nwe can convert a field into an object by thresholding (wind field, storm or hurricane)\nwe can convert objects into a field e.g. by computing the density as a continuous function\n\n\n\nAggregations\n\nwe can aggregate fields, or objects, but do this differently:\npopulation can be summed, temperature cannot (see intensive/extensive properties)"
  },
  {
    "objectID": "index.html#aims-of-modelling",
    "href": "index.html#aims-of-modelling",
    "title": "Modelling Spatiotemporal Processes",
    "section": "Aims of modelling",
    "text": "Aims of modelling\n… could be\n\ncuriousity\nfun: studying models is easier than measuring the world around us\n\nMore scientific aims of modelling are\n\nto learn about the world around us\nto predict the past, current or future, in case where measurement is not feasible.\n\n\nWhat is a model?\n\nconceptual models, e.g. the water cycle (wikipedia:) \n\n\n\n\nthe water cycle, updated\n\n\n\nobject models, such as UML (wikipedia:) \nmathematical models, such as Navier Stokes’ equation, (wikipedia:) \n\n\n\nWhat is a mathematical model?\nA mathematical model is an abstract model that uses mathematical language to describe the behaviour of a system, quoting (Eykhoff 1974) a mathematical model is:\n\na representation of the essential aspects of an existing system (or a system to be constructed) which presents knowledge of that system in usable form\n\nIn the natural sciences, a model is always an approximation, a simplification of reality. If degree of approximation meets the required accuracy, the model is useful, or valid (of value). A validated model does not imply that the model is “true”; more than one model can be valid at the same time.\n\n\n\n\nCamara, Gilberto, Max J Egenhofer, Karine Ferreira, Pedro Andrade, Gilberto Queiroz, Alber Sanchez, Jim Jones, and Lubia Vinhas. 2014. “Fields as a Generic Data Type for Big Spatial Data.” In International Conference on Geographic Information Science, 159–72. Springer. https://link.springer.com/content/pdf/10.1007/978-3-319-11593-1_11.pdf.\n\n\nChatfield, Chris. 2003. The Analysis of Time Series: An Introduction. Chapman; hall/CRC.\n\n\nEykhoff, Pieter. 1974. System Identification. Vol. 14. Wiley London.\n\n\nPebesma, Edzer, and Roger Bivand. 2022. Spatial Data Science, with Applications in R. online. https://r-spatial.org/book/."
  },
  {
    "objectID": "ts1.html#intro-how-do-r-markdown-quarto-files-work",
    "href": "ts1.html#intro-how-do-r-markdown-quarto-files-work",
    "title": "1  Time Series",
    "section": "1.1 Intro: How do R markdown / quarto files work?",
    "text": "1.1 Intro: How do R markdown / quarto files work?\n\nload them with rstudio\nclick the knit or render button\ndebug/step through:\n\nload all code up to here-button\nctrl-Enter: run this line\noutput in console, objects appear in Environment browser"
  },
  {
    "objectID": "ts1.html#time-series-models",
    "href": "ts1.html#time-series-models",
    "title": "1  Time Series",
    "section": "1.2 Time series models",
    "text": "1.2 Time series models\nwe will first look into time series models, because they are + simple + easy to write down + well understood\ntime series models are roughly divided in\n\ntime domain models and, which look at correlations and memory\nfrequency domain models, which focus on periodicities\n\nSpatial equivalents are mostly found in (a), although (b) has spatial equivalences as well (e.g. wavelets).\n\nsome data\nConsider the following process (\\(\\Delta t\\) = 1 min):\n\nload(\"meteo.RData\") # should be available in the current working directory\nls()\n# [1] \"meteo\"\nnames(meteo)\n#  [1] \"ID\"            \"year\"          \"julian.day\"    \"time\"         \n#  [5] \"T.outside\"     \"pressure\"      \"humidity\"      \"X\"            \n#  [9] \"windspeed\"     \"std.dev.\"      \"Wind.dir\"      \"std.dev..1\"   \n# [13] \"TippingBucket\" \"mins\"          \"hours\"         \"date\"         \n# [17] \"T.per\"\nplot(T.outside~date, meteo, type='l', ylab = parse(text = \"Temperature ({}*degree* C)\"), xlab = \"date, 2007\")\ntitle(\"Outside temperature, Hauteville, FR\")\n\n\n\n\n\n\nQuestions\n\nhow can we describe this process in statistical terms?\nhow can we model this process?\n(how) can we predict future observations?\n\n\n\nWhite noise, and AR(\\(n\\))\nPerhaps the simplest time series model is white noise with mean \\(m\\):\n\\[y_t = m + e_t, \\ \\ e_t \\sim N(0,\\sigma^2)\\]\n\\(N(0,\\sigma^2)\\) denoting the normal distribution with mean 0 and variance \\(\\sigma^2\\), and \\(\\sim\\) meaning distributed as or coming from.\n\\(t\\) is the index \\(t=1,2,...,n\\) of the observation, and refers to specific times, which, when not otherwise specified are at regular intervals.\nA white noise process is completely without memory: each observation is independent from its past or future. Plotting independent, standard normal values against their index (the default for plotting a vector in R) shows how a white noise time series would look like:\n\nwhite.noise = rnorm(100)\nplot(white.noise, type='b')\ntitle(\"100 independent observations from N(0,1)\")\n\n\n\n\n\nwhite.noise = rnorm(1000)\nplot(white.noise, type='l')\ntitle(\"1000 independent observations from N(0,1)\")\n\n\n\n\n\nwhite.noise = rnorm(10000)\nplot(white.noise, type='l')\ntitle(\"10000 independent observations from N(0,1)\")\n\n\n\n\nWe can look at the auto-correlation function of a white noise process, and find it is uncorrelated for any lag larger than 0:\n\nplot(acf(white.noise))\n\n\n\n\nCorrelation x and y: \\[\nr(x,y) = \\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_{i}-\\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i-\\bar{x})^2\\sum_{i=1}^n(y_i-\\bar{y})^2}}\n\\]\n\n\nAutocorrelation\nAutocorrelation (or lagged correlation) is the correlation between \\(y_i\\) and \\(y_{i+h}\\), as a function of the lag \\(h\\): \\[\nr(h) = \\frac{\\sum_{i=1}^{n-h}(y_i-\\bar{y})(y_{i+h}-\\bar{y})}{\\sum_{i=1}^n (y_i-\\bar{y})^2}\n\\] with \\(\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i\\)\n\n\nRandom walk\nA simple, next model to look at is that of random walk, where each time step a change is made according to a white noise process: \\[y_t = y_{t-1} + e_t\\] Such a process has memory, and long-range correlation. If we take the first-order differences, \\[y_t - y_{t-1} = e_t\\] we obtain the white noise process.\nFurther, the variance of the process increases with increasing domain (i.e., it is non-stationary)\n\n\nExample random walk:\nWe can compute it as the cumulative sum of standard normal deviates: \\(y_n = \\sum_{i=1}^n e_i\\):\n\n# generate three series:\nrw1 = cumsum(rnorm(5000))\nrw2 = cumsum(rnorm(5000))\nrw3 = cumsum(rnorm(5000))\nplot(rw1, type='l', ylim = range(c(rw1,rw2,rw3)))\nlines(rw2, type='l', col='red')\nlines(rw3, type='l', col='blue')\n\n\n\n\n\nplot(acf(rw3))\n\n\n\n\n\nplot(acf(diff(rw3)))\n\n\n\n\n\n\nMA(1), MA(q)\nLet \\(e_t\\) be a white noise process. A moving average process of order \\(q\\) is generated by \\[y_t = \\beta_0 e_t + \\beta_1 e_{t-1} + ... + \\beta_q e_{t-q}\\]\nNote that the \\(\\beta_j\\) are weights, and could be \\(\\frac{1}{q+1}\\) to obtain an unweighted average. Moving averaging smoothes the white noise series \\(e_t\\).\nMoving average over monthly CO2 measurements on Maunaloa:\n\nplot(co2)\nlines(filter(co2, rep(1/12, 12)), col='blue')\n\n\n\n\nMoving averages over a white noise process:\n\n#set.seed(13531)\ne = rnorm(2000)\nplot(e, type='l')\ne5 = filter(e, rep(1/5, 5))\ne20 = filter(e, rep(1/20, 20))\nlines(e5, col='red')\nlines(e20, col='blue')\n\n\n\n\n\nacf(e)\n\n\n\nacf(e5, na.action = na.pass)\n\n\n\nacf(e20, na.action = na.pass)\n\n\n\n\nWider moving average filters give new processes with + less variation + stronger correlation, over larger lags\n\n\nAR(1), AR(p)\nAn auto-regressive (1) model, or AR(1) model is generated by \\[y_t = \\phi_1 y_{t-1}+e_t\\] and is sometimes called a Markov process. Given knowledge of \\(y_{t-1}\\), observations further back carry no information; more formally: \\[\\Pr(y_t|y_{t-1},y_{t-2},...,y_{t-q}) = \\Pr(y_t|y_{t-1})\\]\n\n\\(\\phi_1 = 1\\) gives random walk, \\(\\phi_1=0\\) gives white noise.\nAR(1) processes have correlations beyond lag 1\nAR(1) processes have non-significant partial autocorrelations beyond lag 1\n\nAs an example, we create (simulate) an AR(1) process with \\(\\phi_1=0.85\\) and \\(e\\) drawn from the standard normal distribution (mean 0, variance 1).\n\nn = 1000\ny = rep(NA_real_, n) # initialise the variable: not needed, but good practice\ny[1] = 0\nfor (i in 2:n) y[i] = 0.85 * y[i-1] + rnorm(1)\nplot(y, type = 'l', main = \"AR(1)\")\n\n\n\nplot(acf(y))\n\n\n\n\nCompare the shape of the acf with that obtained from a MA process: different!\n\n\nAR(p)\n\\[y_t = \\phi_1 y_{t-1}+ \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} + e_t\\] or \\[y_t = \\sum_{j=1}^p \\phi_j y_{t-j}+e_t\\] + The state of \\(y_t\\) does not only depend on \\(y_{t-1}\\), but observations further back contain information + AR(p) have autocorrelations beyond lag p + AR(p) have “zero” partial autocorrelations beyond lag p\n\ny2 = rep(NA_real_, 1000) # initialise the variable: not needed, but good practice\ny2[1] = 0; y2[2] = rnorm(1)\nfor (i in 3:1000) y2[i] = 0.5 * y2[i-1] + 0.15 * y2[i-2] + rnorm(1)\nplot(y2, type = 'l', main = \"AR(2)\")\n\n\n\nplot(acf(y2))\n\n\n\n\n\n\nPartial correlation\n\nCorrelation between \\(y_t\\) and \\(y_{t-2}\\) is simply obtained by plotting both series of length \\(n-2\\), and computing correlation\nLag-2 partial autocorrelation of \\(y_t\\) and \\(y_{t-2}\\), given the value inbetween \\(y_{t-1}\\) is obtained by\ncomputing residuals \\(\\hat{e}_t\\) from regressing of \\(y_t\\) on \\(y_{t-1}\\)\ncomputing residuals \\(\\hat{e}_{t-2}\\) from regressing of \\(y_{t-2}\\) on \\(y_{t-1}\\)\ncomputing the correlation between both residual series \\(\\hat{e}_t\\) and \\(\\hat{e}_{t-2}\\).\nLag-3 partial autocorrelation regresses \\(y_t\\) and \\(y_{t-3}\\) on both intermediate values \\(y_{t-1}\\) and \\(y_{t-2}\\)\netc.\n\nPartial correlation can help reveal what the order of an AR(p) series is.\n\nplot(pacf(e5, na.action = na.omit))\n\n\n\nplot(pacf(e20, na.action = na.omit))\n\n\n\nplot(pacf(y))\n\n\n\nplot(pacf(y2))\n\n\n\n\n\n\nRelation between AR and MA processes\nChatfield (Chatfield 2003) has more details about this. Substitute the AR(1) as follows \\[y_t = \\phi_1 y_{t-1} + e_t\\] \\[y_t = \\phi_1 (\\phi_1 y_{t-2} + e_{t-1}) + e_t\\] \\[y_t = \\phi_1^2 (\\phi_1 y_{t-3} + e_{t-2}) + \\phi_1 e_{t-1} + e_t\\] etc. In the limit, we can write any AR process as an (infinite) MA process, and vice versa.\n\n\n\n\nChatfield, Chris. 2003. The Analysis of Time Series: An Introduction. Chapman; hall/CRC."
  },
  {
    "objectID": "ts2.html#back-to-the-temperature-series",
    "href": "ts2.html#back-to-the-temperature-series",
    "title": "2  Fitting and choosing models",
    "section": "2.1 Back to the temperature series",
    "text": "2.1 Back to the temperature series\n\nload(\"meteo.RData\")\n# plot data and trend:\nplot(T.outside~date,meteo,type='l')\nmeteo$T.per = 18.2-4.9*sin(pi*(meteo$hours+1.6)/12)\nlines(T.per~date,meteo,col='red')"
  },
  {
    "objectID": "ts2.html#removing-the-diurnal-periodicity",
    "href": "ts2.html#removing-the-diurnal-periodicity",
    "title": "2  Fitting and choosing models",
    "section": "2.2 Removing the diurnal periodicity",
    "text": "2.2 Removing the diurnal periodicity\nAssuming this is a sinus function, \\[\\alpha_1 + \\alpha_2 \\sin(t + \\alpha_3),\\] we need non-linear regression (\\(\\alpha_3\\))\n\nattach(meteo) \n# The following object is masked from package:datasets:\n# \n#     pressure\nf = function(x) {\n    sum((T.outside - (x[1] + x[2] * sin(pi * (hours + x[3])/12)))^2)\n}\nnlm(f, c(0, 0, 0)) # zero initial values\n# $minimum\n# [1] 108956\n# \n# $estimate\n# [1] 18.2 -4.9  1.6\n# \n# $gradient\n# [1] -1.60e-06 -8.90e-05  2.18e-04\n# \n# $code\n# [1] 1\n# \n# $iterations\n# [1] 9\n\nWe attached meteo so we can use variable names directly: f has acces to T.outside and hours, without needing them to be passed as arguments (lexical scoping). Without attaching, that would not work.\nIn the next part on optimization we will see what nlm does."
  },
  {
    "objectID": "ts2.html#temperature-anomaly",
    "href": "ts2.html#temperature-anomaly",
    "title": "2  Fitting and choosing models",
    "section": "2.3 Temperature anomaly",
    "text": "2.3 Temperature anomaly\n\nplot(T.outside-T.per~date, meteo, type='l')\ntitle(\"anomaly\")"
  },
  {
    "objectID": "ts2.html#what-can-we-do-with-such-models",
    "href": "ts2.html#what-can-we-do-with-such-models",
    "title": "2  Fitting and choosing models",
    "section": "2.4 What can we do with such models?",
    "text": "2.4 What can we do with such models?\n\ntry to find out which model fits best (model selection)\nlearn how they were/could have been generated\npredict future observations (estimation/prediction/forecasting)\ngenerate similar data ourselves (simulation)"
  },
  {
    "objectID": "ts2.html#how-to-select-a-best-model",
    "href": "ts2.html#how-to-select-a-best-model",
    "title": "2  Fitting and choosing models",
    "section": "2.5 How to select a “best” model?",
    "text": "2.5 How to select a “best” model?\nA possible approach is to find the minimum for Akaike’s Information Criterion (AIC) for ARMA(\\(p,q\\)) models and series of length \\(n\\): \\[AIC = \\log \\hat{\\sigma}^2 + 2(p+q+1)/n\\] with \\(\\hat{\\sigma}^2\\) the estimated residual (noise) variance.\nInstead of finding a single best model using this single criterion, it may be better is to select a small group of “best” models, and look at model diagnostics for each: is the residual white noise? does it have stationary variance?\nEven better may be to keep a number of “fit” models and consider each as (equally?) suitable candidates."
  },
  {
    "objectID": "ts2.html#aic-for-arp-and-as-as-a-function-of-p",
    "href": "ts2.html#aic-for-arp-and-as-as-a-function-of-p",
    "title": "2  Fitting and choosing models",
    "section": "2.6 AIC for AR(p), and as as a function of \\(p\\)",
    "text": "2.6 AIC for AR(p), and as as a function of \\(p\\)\n\nn = 10\naic = numeric(n)\nfor (i in 1:n)\n   aic[i] = arima(T.outside, c(i,0,0))$aic  # AR(i)\naic\n#  [1] -23548 -30235 -30714 -30772 -30815 -30816 -30818 -30818 -30818\n# [10] -30816\nplot(aic[2:10], type='l')"
  },
  {
    "objectID": "ts2.html#anomaly-aic-as-a-function-of-p-for-arp",
    "href": "ts2.html#anomaly-aic-as-a-function-of-p-for-arp",
    "title": "2  Fitting and choosing models",
    "section": "2.7 Anomaly AIC as a function of \\(p\\), for AR(p)",
    "text": "2.7 Anomaly AIC as a function of \\(p\\), for AR(p)\n\nan = T.outside - T.per\naic_an = numeric(n)\nfor (i in 1:n)\n    aic_an[i] = arima(an,c(i,0,0))$aic  # AR(i)\naic_an\n#  [1] -23995 -30320 -30843 -30885 -30945 -30944 -30952 -30958 -30956\n# [10] -30955\nplot(aic_an[2:10], type='l')\n\n\n\n\n\n# Prediction, e.g. with AR(6)\nx = arima(T.outside, c(6,0,0))\n\npltpred = function(xlim, Title) {\n  plot(T.outside, xlim = xlim, type='l')\n  start = nrow(meteo) + 1\n  pr = predict(x, n.ahead = xlim[2] - start + 1)\n  lines(start:xlim[2], pr$pred, col='red')\n  lines(start:xlim[2], pr$pred+2*pr$se, col='green')\n  lines(start:xlim[2], pr$pred-2*pr$se, col='green')\n  title(Title)\n}\npltpred(c(9860, 9900), \"10 minutes\")\n\n\n\npltpred(c(9400, 10000), \"110 minutes\")\n\n\n\npltpred(c(8000, 11330), \"predicting 1 day\")\n\n\n\npltpred(c(1, 19970), \"predicting 1 week\")\n\n\n\n\nplot(T.outside,xlim=c(1,19970), type='l')\nx.an = arima(an, c(6,0,0)) # model the anomaly by AR(6)\nx.pr = as.numeric(predict(x.an, 10080)$pred) \nx.se = as.numeric(predict(x.an, 10080)$se)\nhours.all = c(meteo$hours, max(meteo$hours) + (1:10080)/60)\nT.per = 18.2-4.9*sin(pi*(hours.all+1.6)/12)\nlines(T.per, col = 'blue')\nhours.pr = c(max(meteo$hours) + (1:10080)/60)\nT.pr = 18.2-4.9*sin(pi*(hours.pr+1.6)/12)\nlines(9891:19970, T.pr+x.pr, col='red')\nlines(9891:19970, T.pr+x.pr+2*x.se, col='green')\nlines(9891:19970, T.pr+x.pr-2*x.se, col='green')\ntitle(\"predicting 1 week: periodic trend + ar(6) for anomaly\")\n\n\n\n\nx = arima(T.outside, c(6,0,0))\nplot(T.pr + arima.sim(list(ar = x.an$coef[1:6]), 10080, sd = sqrt(0.002556)), \n    col = 'red', ylab=\"Temperature\")\nlines(18+arima.sim(list(ar = x$coef[1:6]), 10080, sd=sqrt(0.002589)), \n    col = 'blue')\ntitle(\"red: with trend, blue: without trend\")"
  },
  {
    "objectID": "ts2.html#what-can-we-learn-from-this",
    "href": "ts2.html#what-can-we-learn-from-this",
    "title": "2  Fitting and choosing models",
    "section": "2.8 What can we learn from this?",
    "text": "2.8 What can we learn from this?\nPrediction/forecasting:\n\nAR(6) prediction is a compromise between the end of the series and the trend\nthe closer we are to observations, the more similar the prediction is to the nearest (last) observation\nfurther in the future the prediction converges to the trend\nthe more useful (realistic) the trend is, the more realistic the far-into-the-future prediction becomes\nthe standard error of prediction increases when predictions are further in the future."
  },
  {
    "objectID": "ts2.html#a-side-note-fitting-a-phase-with-a-linear-model",
    "href": "ts2.html#a-side-note-fitting-a-phase-with-a-linear-model",
    "title": "2  Fitting and choosing models",
    "section": "2.9 A side note: fitting a phase with a linear model",
    "text": "2.9 A side note: fitting a phase with a linear model\nA phase shift model \\(\\alpha \\sin(x + \\phi)\\) can also be modelled by\n\\(\\alpha sin(x) + \\beta cos(x)\\); this is essentially a re-parameterization. Try the following code:\n\nx = seq(0, 4*pi, length.out = 200) # x plotting range\n\nf = function(dir, x) { # plot the combination of a sin+cos function, based on dir\n    a = sin(dir)\n    b = cos(dir)\n    # three curves:\n    plot(a * sin(x) + b * cos(x) ~ x, type = 'l', asp=1, col = 'green')\n    lines(x, a * sin(x), col = 'red')\n    lines(x, b * cos(x), col = 'blue')\n    # legend:\n    lines(c(10, 10+a), c(2,2), col = 'red')\n    lines(c(10, 10), c(2,2+b), col = 'blue')\n    arrows(x0 = 10, x1 = 10+a, y0 = 2, y1 = 2+b, .1, col = 'green')\n    title(\"red: sin(x), blue: cos(x), green: sum\")\n}\n\nfor (d in seq(0, 2*pi, length.out = 100)) {\n    f(d, x)\n    Sys.sleep(.1)\n}\n\nSo, we can fit the same model by a different parameterization:\n\nnlm(f,c(0,0,0))$minimum\n# [1] 108956\ntt = pi * hours / 12\ng = function(x) sum((T.outside - (x[1]+x[2]*sin(tt)+x[3]*cos(tt)))^2)\nnlm(g,c(0,0,0))\n# $minimum\n# [1] 108956\n# \n# $estimate\n# [1] 18.19 -4.48 -2.00\n# \n# $gradient\n# [1] -0.00033 -0.00241 -0.00124\n# \n# $code\n# [1] 1\n# \n# $iterations\n# [1] 5\n\nwhich is a linear model, identical to:\n\nlm(T.outside~sin(tt)+cos(tt))\n# \n# Call:\n# lm(formula = T.outside ~ sin(tt) + cos(tt))\n# \n# Coefficients:\n# (Intercept)      sin(tt)      cos(tt)  \n#       18.19        -4.48        -2.00"
  },
  {
    "objectID": "ts2.html#next-how-do-we-fit-coefficients",
    "href": "ts2.html#next-how-do-we-fit-coefficients",
    "title": "2  Fitting and choosing models",
    "section": "2.10 Next: how do we fit coefficients?",
    "text": "2.10 Next: how do we fit coefficients?"
  },
  {
    "objectID": "op1.html#linear-systems",
    "href": "op1.html#linear-systems",
    "title": "3  Optimization: linear and non-linear",
    "section": "3.1 linear systems",
    "text": "3.1 linear systems\nTake \\(x_1\\) the example\n\\[\n    \\begin{array}{ll}\n    a x_{11} + b x_{12} & = y_1 \\\\\n    a x_{21} + b x_{22} & = y_2\n    \\end{array}\n\\]\nwith the \\(x\\) and \\(y\\) values known, and \\(a\\) and \\(b\\) unknown. This is similar to fitting a straight line through two points: let \\((x_1,y_1)\\) be the first point and \\((x_2,y_2)\\) be the second, then \\[\n    \\begin{array}{ll}\n    a + b x_1 & = y_1 \\\\\n    a + b x_2 & = y_2\n    \\end{array}\n\\] The approach is substition: rewrite one equations such that isolates \\(a\\) or \\(b\\), and substitute that in the second.\n\nMatrix notation\nWe can rewrite \\[\n    \\begin{array}{ll}\n    a x_{11} + b x_{12} & = y_1\\\\\n    a x_{21} + b x_{22} & = y_2\n    \\end{array}\n\\] as the matrix product \\[\n  \\left[\n  \\begin{array}{ll}\n  x_{11} & x_{12}\\\\\n  x_{21} & x_{22}\n  \\end{array}\n  \\right]\n  \\left[\n  \\begin{array}{l}\n  a \\\\ b\n  \\end{array}\n  \\right]\n  =\n  \\left[\n  \\begin{array}{l}\n  y_1 \\\\ y_2\n  \\end{array}\n  \\right]\n\\] or \\[Xa = y\\]\n\n\nMatrix transposition\nThe transpose of a matrix is the matrix formed when rows and columns are reversed. If \\[A =\n    \\left[\n    \\begin{array}{rr}\n    1 & 4 \\\\\n    2 & -1 \\\\\n    8 & 9 \\\\\n    \\end{array}\n    \\right]\n\\] then it’s transpose, \\[\n    A' = \\left[\n    \\begin{array}{rrr}\n    1 & 2 & 8 \\\\\n    4 & -1 & 9 \\\\\n    \\end{array}\n    \\right]\n\\] (and may be written as \\(A^T\\))\n\n\nMatrix inverse and identity\nThe identity matrix is square (nr of rows equals nr of columns), has ones on the diagona (for which the row number equals the column number) and zeroes elsewhere. E.g. the \\(3 \\times 3\\) identity \\[\n    I = \\left[\n    \\begin{array}{lll}\n    1 & 0 & 0 \\\\\n    0 & 1 & 0 \\\\\n    0 & 0 & 1 \\\\\n    \\end{array}\n    \\right]\n\\] The inverse of a square matrix \\(X\\), \\(X^{-1}\\), is defined by the products \\[X^{-1}X = I\\] and \\[X X^{-1}=I\\]\nSuppose we have \\(n\\) equations with \\(p\\) unknowns: \\[\n    \\begin{array}{cccccc}\n    a_1 x_{11} + a_2 x_{12} + & ... & + & a_p x_{1p} & = & y_1 \\\\\n    a_1 x_{21} + a_2 x_{22} + & ... & + & a_p x_{2p} & = & y_2 \\\\\n    \\vdots & \\ddots & & \\vdots & & \\vdots \\\\\n    a_1 x_{n1} + a_2 x_{n2} + & ... & + & a_p x_{np} & = & y_n\n    \\end{array}\n\\] we can rewrite this in matrix notation as \\(Xa=y\\), with \\(x_{ij}\\) corresponding to element \\((i,j)\\) (row i, column j) in \\(X\\), having \\(n\\) rows and \\(p\\) columns; \\(a\\) and \\(y\\) column vectors having \\(p\\) and \\(n\\) elements, respectively. Now, \\(X\\) and \\(y\\) are known, and \\(a\\) is unknown. \\(a\\) Solutions:\n\nif \\(p > n\\), there is no single solution\nif \\(p = n\\) and \\(X\\) is not singular, then \\(a = X^{-1}y\\)\nif \\(p < n\\) we have an overdetermined system, and may e.g. look for\n\na least square (best approximating) solution."
  },
  {
    "objectID": "op1.html#linear-least-squares-solution",
    "href": "op1.html#linear-least-squares-solution",
    "title": "3  Optimization: linear and non-linear",
    "section": "3.2 Linear least squares solution",
    "text": "3.2 Linear least squares solution\nIf \\(p < n\\), a solution usually does not exist: try fitting a straight line through three or more arbitrary points.\nNow rewrite \\(Xa = y\\) as \\(y=Xb+e\\), with \\(e\\) the distance (in \\(y\\)-direction) from the line. If we want to minimize the sum of squared distances, then we need to find \\(b\\) for which \\(R=\\sum_{i=1}^n e_i^2\\) is minimum. In matrix terms, \\(R = (y-Xb)'(y-Xb)\\) with \\('\\) denoting transpose (row/col swap). \\[\\frac{\\delta R}{\\delta b} = 0\\] \\[\\frac{\\delta (y-Xb)'(y-Xb)}{\\delta b} = 0\\] \\[\\frac{\\delta (y'y - (Xb)'y- y'(Xb) + (Xb)'Xb)}{\\delta b} = 0\\] now you should first note that \\((Xb)'=b'X'\\), and second that \\(b'X'y=y'Xb\\) because these are scalars. Then, \\[-2X'y + 2X'Xb = 0\\] \\[X'Xb = X'y\\] \\[b = (X'X)^{-1}X'y\\] this yields the least squares solution for \\(b\\); the solution equations are called the normal equations.\n\nThe practice of solving systems\nwhen we write \\[A x = b\\] with known \\(A\\) and \\(b\\) and unknown \\(x\\), the solution is \\[x = A^{-1}b\\] In practice however, we do not need to compute \\(A^{-1}\\), but can directly solve for \\(x\\). This is much cheaper.\n\nm=matrix(0,3000,3000)\ndiag(m)=1\nsystem.time(x <- solve(m))\n#    user  system elapsed \n#    6.43    0.61    1.03\nsystem.time(x <- solve(m,rep(0,3000)))\n#    user  system elapsed \n#   1.808   0.480   0.325\n\n\nX=cbind(c(1,1,1),c(1,2,3))\nX\n#      [,1] [,2]\n# [1,]    1    1\n# [2,]    1    2\n# [3,]    1    3\ny = c(1,0,2)\nsolve(t(X) %*% X, t(X) %*% y)\n#      [,1]\n# [1,]  0.0\n# [2,]  0.5\nplot(X[,2], y, xlim = c(0,3), asp = 1)\nabline(lm(y~X[,2]))\n\n\n\nlm(y~X[,2])\n# \n# Call:\n# lm(formula = y ~ X[, 2])\n# \n# Coefficients:\n# (Intercept)       X[, 2]  \n#   -3.85e-16     5.00e-01"
  },
  {
    "objectID": "op1.html#non-linear-optimization",
    "href": "op1.html#non-linear-optimization",
    "title": "3  Optimization: linear and non-linear",
    "section": "3.3 Non-linear Optimization",
    "text": "3.3 Non-linear Optimization\n\none-dimensional search on a unimodal function: golden search\nnon-linear least squares: the Gauss Newton algorithm\nprobabilistic methods: global search\nMetropolis-Hastings\nSimulated Annealing\n\n\nGolden search\nGolden ratio: \\[\n    \\frac{x_1}{x_2} = \\frac{x_2}{x_1+x_2}\n\\] Solution (check): if \\(x_1=1\\), then \\(x_2\\approx1.618\\) or \\(x_2\\approx0.618\\)\nFound in: art, sculpture, geometry (pentagrams), Egyptian pyramides, architecture, nature, A4 paper, …\n\n\nMinimum outside current section\n\n\n\ngolden1\n\n\n\n\nMinimum inside current section\n\n\n\ngolden2\n\n\n\n\n\ngolden3\n\n\n\n\n\ngolden4\n\n\n\n\n\ngolden5\n\n\n\n\n\ngolden6\n\n\n\n\nthe algorithm\nRecursive zooming:\n\nfind three GR points, a, b and c such that the minimum lies within a and c\nput a point d in the largest section according to GR, with the smallest interval closest to the smallest value\n(In case of adbc) determine whether the mininum is between a and b or d and c\ncontinue with either adb or dbc as if it were abc, unless we’re sufficiently close (in terms of our goal, or of numerical resolution)\n\n\n\nCombined linear and golden search\nSpherical variogram with nugget has three parameters: nugget \\(c_0\\), (partial) sill \\(c_1\\) and range \\(a\\): \\[\n\\gamma(h) = \\left\\{\n\\begin{array}{ll}\n          0 & \\mbox{if}\\ \\  h = 0 \\\\\n          c_0 + c_1 f(a,h) & \\mbox{if}\\ \\  h > 0 \\\\\n\\end{array}\n\\right.\n\\] with \\[f(a, h)= \\left\\{\n\\begin{array}{ll}\n\\frac{3h}{2a}-\\frac{1}{2}(\\frac{h}{a})^3 & \\mbox{if} \\ \\ 0 \\le h \\le a \\\\\n1 & \\mbox{if} \\ \\ h > a \\\\\n\\end{array}\n\\right.\n\\]\n\n\nApproach:\nProvide an initial estimate \\(a_0\\); then iterate: 1. given current fit for \\(a\\), fit the linear coefficients \\(c_0\\) and \\(c_1\\) 2. given this fit, do golden search for \\(a\\) until convergence (vector \\((a,c_0,c_1)\\) does not move).\n\nlibrary(sp)\ndata(meuse)\ncoordinates(meuse) = ~x+y\nlibrary(gstat)\nv = variogram(log(zinc)~1, meuse)\nm = vgm(0.5, \"Sph\", 700, 0.1)\nplot(v)\n\n\n\nplot(v, vgm(0.5, \"Sph\", 700, 0.1))\n\n\n\nplot(v, fit.variogram(v, vgm(0.5, \"Sph\", 700, 0.1)))"
  },
  {
    "objectID": "op1.html#gauss-newton",
    "href": "op1.html#gauss-newton",
    "title": "3  Optimization: linear and non-linear",
    "section": "3.4 Gauss-Newton",
    "text": "3.4 Gauss-Newton\nGolden search may be used for any criterion, e.g. \\(f(x)=\\sum_{i=1}^n g_i(x)^p\\) for any chosen \\(p\\). If we limit ourselves to least squares (i.e., \\(p=2\\)) and want to generalize this for higher dimensional (i.e., multiple parameter) \\(x\\) (e.g. \\(x=[x_1,...,x_q]'\\)) we may use the Gauss-Newton algorithm (non-linear least squares).\n\nthe Gauss-Newton algorithm\nProblem: given a model \\(y=g(X,\\theta)+e\\) find \\[\\mbox{min}_\\theta \\sum (y - g(X,\\theta))^2\\] Let \\(f_i(\\theta)=y_i - g(X_i,\\theta)\\), so we minimize \\(R=\\sum_{i=1}^n (f_i(\\theta))^2\\)\nThis is a problem from space \\((1 \\times n)\\) to \\((1 \\times m)\\)\nGiven a starting value \\(\\theta^0\\) we search the direction of steepest descent in terms of \\(R\\), using first order derivatives of \\(R\\) towards \\(\\theta\\). By iteration, from \\(\\theta^k\\) we find \\(\\theta^{k+1}\\) by \\[\\theta^{k+1}=\\theta^k + \\delta^k\\] until we have convergence.\nLet the Jakobian be \\[\nJ_f(\\theta^k) =\n\\left[\n\\begin{array}{ccccc}\n\\frac{\\delta f_1(\\theta^k)}{\\delta\\theta_1} & ... &  \\frac{\\delta f_1(\\theta^k)}{\\delta\\theta_m} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\delta f_n(\\theta^k)}{\\delta\\theta_1} & ... &  \\frac{\\delta f_n(\\theta^k)}{\\delta\\theta_m} \\\\\n\\end{array}\n\\right]\n\\] In \\[\\theta^{k+1}=\\theta^k + \\delta^k\\] we find \\(\\delta^k\\) by solving \\[\nJ_f(\\theta_k)'J_f(\\theta_k) \\delta^k = - J_f(\\theta_k)'f(\\theta^k)\n\\] What if \\(\\delta f_n(\\theta^k)/ \\delta \\theta\\) is unknown?\n\n\nGauss-Newton and the Normal equations\nRecall that in multiple linear regression, with \\(y=X\\theta+e\\) the solution is given by the normal equations \\[X'X\\theta = X'y\\] Note that here, the Jacobian of \\(y-X\\theta\\) is \\(-X\\), so if we take (arbitrarily) \\(\\theta_0 = (0,0,...,0)'\\), then \\[\nJ_f(\\theta_k)'J_f(\\theta_k) \\delta^k = - J_f(\\theta_k) f(\\theta^k)\n\\] yields after one step the final solution \\(\\delta^1=\\theta\\), as \\((-X)'(-X)\\delta=X'y\\).\nOther starting points yield the same solution for \\(\\theta\\).\nFurther steps will not improve it (i.e., yield \\(\\delta^k=0\\)).\nSee also the Gradient descent wikipedia site."
  },
  {
    "objectID": "op2.html#problems-with-steepest-descent",
    "href": "op2.html#problems-with-steepest-descent",
    "title": "4  Optimization: stochastic methods",
    "section": "4.1 Problems with steepest descent",
    "text": "4.1 Problems with steepest descent\n\n(see previous slide:) steepest descent may be very slow\nMain problem: a minimum may be local, other initial values may result in other, better minima\nCure: apply Gauss-Newton from many different starting points (cumbersome, costly, cpu intensive)\nGlobal search:\n\napply a grid search – curse of dimensionality. E.g. for three parameters, 50 grid nodes along each direction: \\(50^3= 125000\\)\napply random sampling (same problem)\nuse search methods that not only go downhill:\n\nMetropolis-Hastings (sampling)\nSimulated Annealing (optimizing)"
  },
  {
    "objectID": "op2.html#metropolis-hastings",
    "href": "op2.html#metropolis-hastings",
    "title": "4  Optimization: stochastic methods",
    "section": "4.2 Metropolis-Hastings",
    "text": "4.2 Metropolis-Hastings\nWhy would one want probabilistic search?\n\nglobal–unlikely areas are searched too (with small probability)\na probability distribution is richer than a point estimate:\n\nGauss-Newton provides an estimate of \\(\\hat\\theta\\) of \\(\\theta\\), given data \\(y\\). What about the estimation error \\(\\hat\\theta - \\theta\\)? Second-order derivatives give approximations to standard errors, but not the full distribution.\nWe explain the simplified version, the Metropolis algorithm\n\nMetropolis algorithm\nGiven a point in parameter space \\(\\theta\\), say \\(x_t = (\\theta_{1,t},...,\\theta_{p,t})\\) we evaluate whether another point, \\(x'\\) is a reasonable alternative. If accepted, we set \\(x_{t+1}\\leftarrow x'\\); if not we keep \\(x_t\\) and set \\(x_{t+1}\\leftarrow x_t\\).\n\nif \\(P(x') > P(x_t)\\), we accept \\(x'\\) and set \\(x_{t+1}=x'\\)\nif \\(P(x') < P(x_t)\\), then\n\nwe draw \\(U\\), a random uniform value from \\([0,1]\\), and\naccept \\(x'\\) if \\(U < \\frac{P(x')}{P(x_t)}\\)\n\n\nOften, \\(x'\\) is drawn from some normal distribution centered around \\(x_t\\): \\(N(x_t,\\sigma^2 I)\\). Suppose we accept it always, then \\[x_{t+1}=x_t + e_t\\] with \\(e_t \\sim N(0,\\sigma^2 I)\\). Looks familiar?\n\n\nBurn-in, tuning \\(\\sigma^2\\)\n\nWhen run for a long time, the Metropolis (and its generalization Metropolis-Hastings) algorithm provide a correlated sample of the parameter distribution\nM and MH algorithms provide Markov Chain Monte Carlo samples; another even more popular algorithm is the Gibb’s sampler (WinBUGS).\nAs the starting value may be quite unlikely, the first part of the chain (burn-in) is usually discarded.\nif \\(\\sigma^2\\) is too small, the chain mixes too slowly (consecutive samples are too similar, and do not describe the full PDF)\nif \\(\\sigma^2\\) is too large, most proposal values are not accepted\noften, during burn-in, \\(\\sigma^2\\) is tuned such that acceptance rate is close to 60%.\nmany chains can be run, using different starting values, in parallel\n\n\n\nLittle mixing (too low acceptance rate):\n\n\n\nBetter mixing:\n\n\n\nStill better mixing:\n\n\n\nLittle mixing (too high acceptance rate: too much autocorrelation)\n\n\n\nLikelihood ratio – side track\nFor evaluating acceptance, the ratio \\(\\frac{P(x')}{P(x_t)}\\) is needed, not the individual values.\nThis means that \\(P(x')\\) and \\(P(x_t)\\) are only needed up to a normalizing constant: if we have values \\(aP(x')\\) and \\(aP(x_t)\\), than that is sufficient as \\(a\\) cancels out.\nThis result is key to the reason that MCMC and M-H are the work horse in Bayesian statistics, where \\(P(x')\\) is extremely hard to find because it calls for the evaluation of a very high-dimensional integral (the normalizing constant that makes sure that \\(P(\\cdot)\\) is a probability) but \\(aP(x')\\), the likelihood of \\(x\\) given data, is much easier to find!\n\n\nLikelihood function for normal distribution\nNormal probability density function: \\[Pr(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})\\] Likelihood, Multivariate; independent observations: \\[Pr(x_1,x_2,...,x_p;\\mu,\\sigma) = \\prod_{i=1}^p Pr(x_i)\\] which is proportional to \\[\\exp(-\\frac{\\sum_{i=1}^n(x_i-\\mu)^2}{2\\sigma^2})\\]"
  },
  {
    "objectID": "op2.html#simulated-annealing",
    "href": "op2.html#simulated-annealing",
    "title": "4  Optimization: stochastic methods",
    "section": "4.3 Simulated annealing",
    "text": "4.3 Simulated annealing\nSimulated Annealing is a related global search algorithm, does not sample the full parameter distribution but searches for the (global) optimimum.\nThe analogy with annealing, the forming of crystals in a slowly cooling substance, is the following:\nThe current solution is replaced by a worse “nearby” solution with a certain probability that depends on the the degree to which the “nearby” solution is worse, and on the temperature of the cooling process; this temperature slowly decreases, allowing less and smaller changes.\nAt the start, temperature is large and search is close to random; when temperature decreases search is more and more local and downhill. Random, uphill jumps prevent SA to fall into a local minimum.\nA related algorithm (stochastic optimization) is the Genetic Algorithm."
  },
  {
    "objectID": "si1.html#taking-a-step-back",
    "href": "si1.html#taking-a-step-back",
    "title": "5  Simple approaches",
    "section": "5.1 Taking a step back",
    "text": "5.1 Taking a step back\nWhy do we need models?\n\nto understand relations or processes\nto assess (predict, forcast, map) something we do or did not measure and cannot see\nto assess the consequence of decisions (scenarios) where we cannot measure\n\n\nA sample dataset\nThe meuse dataset is loaded from package sp:\n\nlibrary(sf)\n# Linking to GEOS 3.10.2, GDAL 3.4.3, PROJ 8.2.1; sf_use_s2() is TRUE\ndata(meuse.riv, package = \"sp\")\ndata(meuse, package = \"sp\")\nmeuse.sf = st_as_sf(meuse, coords = c(\"x\", \"y\"))\nmeuse.sr = st_sfc(st_polygon(list(meuse.riv)))\nbr = c(0, 100,200,400,700,1200,2000)\nplot(meuse.sf[\"zinc\"], pch = 16,\n    breaks = br, at = br, key.pos = 4,\n    main = \"zinc, ppm\", reset = FALSE)\nplot(meuse.sr, col = \"lightblue\", add = TRUE)\n\n\n\n\n\n\nThiessen “polygons”, 1-NN\n\n\nlibrary(gstat)\ndata(meuse.grid, package = \"sp\") # data.frame\nlibrary(stars)\n# Loading required package: abind\nmeuse.grid = st_as_stars(meuse.grid) # makes it a raster\nmeuse.grid\n# stars object with 2 dimensions and 5 attributes\n# attribute(s):\n#     part.a         part.b          dist        soil      \n#  Min.   :0      Min.   :0      Min.   :0      1   :1665  \n#  1st Qu.:0      1st Qu.:0      1st Qu.:0      2   :1084  \n#  Median :0      Median :1      Median :0      3   : 354  \n#  Mean   :0      Mean   :1      Mean   :0      NA's:5009  \n#  3rd Qu.:1      3rd Qu.:1      3rd Qu.:0                 \n#  Max.   :1      Max.   :1      Max.   :1                 \n#  NA's   :5009   NA's   :5009   NA's   :5009              \n#   ffreq     \n#  1   : 779  \n#  2   :1335  \n#  3   : 989  \n#  NA's:5009  \n#             \n#             \n#             \n# dimension(s):\n#   from  to offset delta x/y\n# x    1  78 178440    40 [x]\n# y    1 104 333760   -40 [y]\n\nmeuse.th = idw(zinc~1, meuse.sf, meuse.grid, nmax = 1)\n# [inverse distance weighted interpolation]\n\nplot(meuse.th[1], nbreaks = 29, col = sf.colors(28),\n    main = \"Zinc, 1-nearest neighbour\", reset = FALSE)\nplot(st_geometry(meuse.sf), col = 3, cex=.5, add = TRUE)\n\n\n\n\n\n\nZinc concentration vs. distance to river\n\nplot(log(zinc)~sqrt(dist), meuse.sf)\nabline(lm(log(zinc) ~ sqrt(dist), meuse.sf))\n\n\n\n\n\n\nZinc conc. vs. distance to river: map of linear trend\n\nmeuse.grid$sqrtdist = sqrt(meuse.grid$dist)\nplot(meuse.grid[\"sqrtdist\"], col = sf.colors(), breaks = \"equal\")\n\n\n\n\n\n\nInverse distance weighted interpolation\nUses a weighted average: \\[\\hat{Z}(s_0) = \\sum_{i=1}^n \\lambda_i Z(s_i)\\] with \\(s_0 = \\{x_0, y_0\\}\\), or \\(s_0 = \\{x_0, y_0, \\mbox{depth}_0\\}\\) weights inverse proportional to power \\(p\\) of distance: \\[\\lambda_i = \\frac{|s_i-s_0|^{-p}}{\\sum_{i=1}^n |s_i-s_0|^{-p}}\\] * power \\(p\\): tuning parameter * if for some \\(i\\), \\(|s_i-s_0| = 0\\), then \\(\\lambda_i = 1\\) and other weights become zero * \\(\\Rightarrow\\) exact interpolator\n\n\nEffect of power \\(p\\)\n\nset.seed(13531)\npts = data.frame(x = 1:10, y = rep(0,10), z = rnorm(10)*3 + 6)\npts.sf = st_as_sf(pts, coords = c(\"x\", \"y\"))\nxpts = 0:1100 / 100\ngrd = data.frame(x = xpts, y = rep(0, length(xpts)))\ngrd = st_as_sf(grd, coords = c(\"x\", \"y\"))\nplot(z ~ x, as.data.frame(pts), xlab ='location', ylab = 'attribute value')\nlines(xpts, idw(z~1, pts.sf, grd, idp = 2)$var1.pred)\n# [inverse distance weighted interpolation]\nlines(xpts, idw(z~1, pts.sf, grd, idp = 5)$var1.pred, col = 'darkgreen')\n# [inverse distance weighted interpolation]\nlines(xpts, idw(z~1, pts.sf, grd, idp = .5)$var1.pred, col = 'red')\n# [inverse distance weighted interpolation]\nlegend(\"bottomright\", c(\"2\", \".5\", \"5\"), lty = 1, \n    col=c(\"black\", \"red\", \"darkgreen\"), title = \"invese distance power\")\n\n\n\n\n\n\nTime series versus spatial data\nDifferences:\n\nspatial data live in 2 (or 3) dimensions\nthere’s no past and future\nthere’s no simple conditional independence (AR)\n\nCorrespondences\n\nnearby observations are more alike (auto-correlation)\nwe can form moving averages\ncoordinate reference systems are a bit like time zones and DST\n\n\n\nWhat information do we have?\n\nWe have measurements \\(Z(x)\\), with \\(x\\) two-dimensional (location on the map)\nwe have \\(x\\) and \\(y\\)\nwe may have land use data\nwe may have soil type or geological data\nwe may have remote sensing imagery\nwe may have all kinds of relevant information, related to processes that cause (or result from) \\(Z(x)\\)\nwe have google maps\n\nWe don’t want to ignore anything important\n\n\nRegression or correlation?\n(Correlation between two different variables, no auto-correlation:)\n\nn = 500\nx = rnorm(n)\ny = rnorm(n)\nR = matrix(c(1,0,0,1),2,2)\nR\n#      [,1] [,2]\n# [1,]    1    0\n# [2,]    0    1\nplot(cbind(x,y) %*% chol(R))\ntitle(\"zero correlation\")\n\n\n\nR[2,1] = R[1,2] = 0.5\nplot(cbind(x,y) %*% chol(R))\ntitle(\"correlation 0.5\")\n\n\n\nR[2,1] = R[1,2] = 0.9\nplot(cbind(x,y) %*% chol(R))\ntitle(\"correlation 0.9\")\n\n\n\nR[2,1] = R[1,2] = 0.98\nplot(cbind(x,y) %*% chol(R))\ntitle(\"correlation 0.98\")\n\n\n\n\n\n\nCorrelation vs. regression:\nCorrespondences:\n\nboth are in the “classic statistics” book, and may involve hypothesis testing\nboth deal with two continuous variables\nboth look at (first order) linear relations\nwhen correlation is significant, the regression slope is significant\n\nDifferences:\n\nRegression distinguishes \\(y\\) from \\(x\\): \\(y\\) depends on \\(x\\), not reverse;\nthe line \\(y=ax+b\\) is not equal to the line \\(x = cy + d\\)\nCorrelation is symmetric: \\(\\Cor(x,y)=\\Cor(y,x)\\)\nCorrelation coefficient is unitless and within \\([-1,1]\\), regression coeficients have data units\nRegression is concerned with prediction of \\(y\\) from \\(x\\).\n\n\n\nThe power of regression models for spatial prediction\n… is hard to overestimate. Regression and correlation are the fork and knife of statistics.\n\nlinear models have endless application: polynomials, interactions, nested effects, ANOVA/ANCOVA models, hypothesis testing, lack of fit testing, …\npredictors can be transformed non-linearly\nlinear models can be generalized: logistic regression, Poisson regression, …, to cope with discrete data (0/1 data, counts, log-normal)\nmany derived techniques solve one particular issue in regression, e.g.:\n\nridge regression solves collinearity (extreme correlation among predictors)\nstepwise regression automatically selects “best” models among many candidates\nclassification and regression trees\n\n\n\n\nWhy is regression difficult in spatial problems?\nRegression models assume independent observations. Spatial data are always to some degree spatially correlated.\nThis does not mean we should discard regression, but rather think about\n\nto which extent is an outcome dependent on independence?\nto which extent is regression robust agains a violated assumption of independent observations?\nto which extent is the assumption violated? (how strong is the correlation)\n\n\n\nWhat is spatial correlation?\nWaldo Tobler’s first “law” in geography: “Everything is related to everything else, but near things are more related than distant things.” (Tobler 1970)\nSetting aside whether Tobler was the first to acknowledge this, and also whether the expression can be called a “law”, we wonder how being related can be expressed?\n\n\n\n\nTobler, W. R. 1970. “A Computer Movie Simulating Urban Growth in the Detroit Region.” Economic Geography 46: 234–40. https://doi.org/10.2307/143141."
  },
  {
    "objectID": "si2.html",
    "href": "si2.html",
    "title": "6  Spatial prediction",
    "section": "",
    "text": "\\[\n\\newcommand{\\E}{{\\rm E}}       % E expectation operator\n\\newcommand{\\Var}{{\\rm Var}}   % Var variance operator\n\\newcommand{\\Cov}{{\\rm Cov}}   % Cov covariance operator\n\\newcommand{\\Cor}{{\\rm Corr}}\n\\]\n\nWhat is spatial correlation?\nIdea from time series: look at lagged correlations, and the \\(h\\)-scatterplot.\nWhat is it? Plots of (or correlation between) \\(Z(s)\\) and \\(Z(s+h)\\), where \\(s+h\\) is \\(s\\), shifted by \\(h\\) (time distance, spatial distance).\n\n\nRandom variables: expectation, variance, covariance\nRandom variable: \\(Z\\) follows a probability distribution, specified by a density function \\(f(z)= \\Pr(Z=z)\\) or a distribution function \\(F(z)=\\Pr(Z \\le z)\\)\nExpectation: \\(\\E(Z) = \\int_{-\\infty}^{\\infty} f(s)ds\\) – center of mass, mean.\nVariance: \\(\\Var(Z)=\\E(Z-\\E(Z))^2\\) – mean squared distance from mean; measure of spread; square root: standard deviation of \\(Z\\).\nCovariance: \\(\\Cov(X,Y)=\\E((X-\\E(X))(Y-\\E(Y)))\\) – mean product; can be negative; \\(\\Cov(X,X)=\\Var(X)\\).\nCorrelation: \\(r_{XY}=\\frac{\\Cov(X,Y)}{\\sqrt{\\Var(X)\\Var(Y)}}\\) – normalized \\([-1,1]\\) covariance. -1 or +1: perfect correlation.\n\n\nNormal distribution\n\nunivariate: If \\(Z\\) follows a normal distribution, its probability distribution is completely characterized by its mean \\(E(Z)=\\mu\\) and variance \\(\\Var(Z)=\\sigma^2\\)\nmultivariate: If the vector \\(Z=(Z_1,Z_2,...,Z_p)\\) follows a multivariate normal distribution, its marginal distributions are univariate normal, and its joint probability distribution is completely characterized by the mean vector \\(E(Z)=\\mu=(\\mu_1,...\\mu_p)\\) and covariance matrix \\(V\\), of which element \\((i,j)\\) equals \\(\\Cov(Z_i,Z_j)\\)\ncovariance matrices have variances on the diagonal\n\n\n\nHow can correlation help prediction?\nProblem:\n\n\n\nQuestions\nGiven observation \\(z(s_1)\\), how to predict \\(z(s_0)\\)?\n\nWhat is the best predicted value at \\(s_0\\), \\(\\hat{z}(s_0)\\)?\nHow can we compute a measure of error for \\(\\hat{z}(s_0)-z(s_0)\\)?\nCan we compute e.g.~95% prediction intervals for the unknown \\(z(s_0)\\)?\n\nObviously, given only \\(z(s_1)\\), the best predictor for \\(z(s_0)\\) is \\(\\hat{z}(s_0)=z(s_1)\\).\nBut what is the error variance, i.e. \\(\\mbox{Var}(\\hat{z}(s_0)-z(s_0))\\)?\n\n\nEstimation error\nLet both \\(z(s_1)\\) and \\(z(s_0)\\) come from a field that has variance 1, i.e. \\(\\mbox{Var}(z(s_0)) = \\mbox{Var}(z(s_1))=1\\), and that has a constant mean: \\(\\mbox{E}(z(s_0)) = \\mbox{E}(z(s_1))=m\\)\nThen, \\[\\mbox{Var}(\\hat{z}(s_0)-z(s_0)) = \\mbox{Var}(z(s_1)-z(s_0))\\]\nAs both have the same mean, this can be written as \\[\\mbox{E}(\\hat{z}(s_0)-z(s_0))^2 = \\mbox{Var}(z(s_1)) + \\mbox{Var}(z(s_0)) - 2\\mbox{Cov}(z(s_1),z(s_0))\\]\nAs both have variance 1, this equals \\(2(1-r)\\) with \\(r\\) the correlation between \\(z(s_0)\\) and \\(z(s_1)\\). Examples follow.\n\n\nSuppose we know the mean\nIf we know the mean \\(\\mu\\), it may be a good idea to use a compromise between the observation and the mean, e.g. \\[\\hat{z}(s_0) = (1-r) \\mu + r z(s_1)\\]\n\n\nNext problems…\n\n\n\n\nWhat is Geostatistical Interpolation?\nGeostatistical interpolation (kriging) uses linear predictors \\[\\hat{z}(s_0) = \\sum_{i=1}^n \\lambda_i z(s_i)\\] with weights chosen such that * the interpolated values is unbiased: \\(\\mbox{E}(\\hat{z}(s_0)-z(s_0))=0\\) and * has mininum variance: \\(\\mbox{Var}(\\hat{z}(s_0)-z(s_0))\\) is at minimum.\nAll that is needed is variances and correlations.\n\n\nThe quadratic form\nWe will not consider single random variables, but rather large collections of them. In fact, we will consider each observation \\(z(s_i)\\) as a realisation (outcome) of a random variable \\(Z(s_i)\\), and consider the \\(Z\\) variable at all other locations also as separate random variables, say \\(Z(s_0)\\) for any \\(s_0\\) in the domain of interest.\nLet \\(Z = [Z(s_1)\\ Z(s_2)\\ ...\\ Z(s_n)]'\\) then \\(\\Var(Z)=V\\) is the covariance matrix of vector \\(Z\\), with \\(i,j\\)-th element \\(\\Cov(Z(s_i),Z(s_j))\\), implying it has variances on the diagonal.\nThen, it is easy to show that for non-random weights \\(\\lambda = [\\lambda_1 ... \\lambda_n]'\\) the quadratic form \\(\\lambda'Z = \\sum_{i=1}^n \\lambda_i Z(s_i)\\) has variance \\[ \\Var(\\lambda'Z) = \\lambda' \\Var(Z) \\lambda =\n\\sum_{i=1}^n \\sum_{j=1}^n \\lambda_i \\lambda_j \\Cov(Z(s_i),Z(s_j)) = \\lambda'V\\lambda\\]\n\n\nWhy do we need this?\nWhen we predict (interpolate), we’re forming linear combinations, \\(\\sum_{i=1}^n \\lambda_i Z(s_i)\\), and want to know the variance of \\(\\sum_{i=1}^n \\lambda_i Z(s_i) - Z(s_0)\\), the interpolation error variance. Only then can we find weights such that it is minimum.\nWhat is the scalar \\(\\Var(\\sum_{i=1}^n \\lambda_i Z(s_i)-Z(s_0))\\)? Write as\n\\[\\Var(\\lambda'Z - Z(s_0)) = \\Var(\\lambda'Z) + \\Var(Z(s_0)) - 2\\Cov(\\lambda'Z,Z(s_0))\\] \\[=\\lambda'V\\lambda + \\sigma_0^2 + \\sum_{i=1}^n \\lambda_i \\Cov(Z(s_i),Z(s_0)) \\] with \\(\\sigma_0^2 = \\Var(Z(s_0))\\)\nSo, we need variances of all \\(Z(s_i)\\), including for all \\(s_0\\), and all covariances between pairs \\(Z(s_i)\\) and \\(Z(s_j)\\), including all \\(s_0\\).\n\n\nSuppose we know all that\nKriging: find weights \\(\\lambda\\) such that \\(\\Var(Z(s_0)-\\hat{Z}(s_0))= \\Var(Z(s_0)-\\sum_{i=1}^n\\lambda_i Z(s_i))\\) is minimized, and we have the best (minimum variance) linear predictor.\nBest linear prediction weights: Let \\(V=\\Var(Z)\\ \\ (n\\times n)\\) and \\(v=\\Cov(Z(s_0),Z)\\ \\ (n\\times 1)\\), and scalar \\(\\Var(Z(s_0)) = \\sigma^2_0\\).\nExpected squared prediction error \\(\\E(Z(s_0)-\\hat{Z}(s_0))^2 = \\sigma^2(s_0)\\)\nReplace \\(Z\\) with \\(Z-\\mu\\) (or assume \\(\\mu=0\\))\n\\[\\sigma^2(s_0) = \\E(Z(s_0)-\\lambda ' Z)^2 =\n\\E(Z(s_0))^2 - 2 \\lambda '\\E(Z(s_0) Z)+\\lambda'\\E(Z Z')\\lambda \\]\n\\[ = \\Var(Z(s_0)) - 2 \\lambda'\\Cov(Z(s_0),Z) + \\lambda'\\Var(Z)\\lambda\n= \\sigma^2_0 - 2 \\lambda'v + \\lambda'V\\lambda \\]\nChoose \\(\\lambda\\) such that \\(\\frac{\\delta \\sigma^2(s_0)}{\\delta\\lambda} = -2 v' + 2\\lambda'V = 0\\)\n\\(\\lambda' = v' V^{-1}\\)\nBLP/Simple kriging:\n\n\\(\\hat{Z}(s_0) = \\mu + v'V^{-1} (Z-\\mu)\\)\n\\(\\sigma^2(s_0) = \\sigma^2_0 - v'V^{-1}v\\)\n\n\nlibrary(sf) # st_distance()\ncov = function(h) exp(-h)\n\nsk = function(data, newdata, mu, cov) {\n    V = cov(st_distance(data))\n    v = cov(st_distance(data, newdata))\n    mu + t(v) %*% solve(V, data[[1]] - mu)\n}\n\n# prediction location at (0,1):\nnewdata = st_as_sf(data.frame(x = 0, y = 1), coords = c(\"x\", \"y\"))\n\n# observation location at (1,1), with attribute value (y) 3:\ndata = st_as_sf(data.frame(x = 1, y = 1, z = 3), coords = c(\"x\", \"y\"))\n\nsk(data, newdata, 0, cov) # mu = 0\n#      [,1]\n# [1,]  1.1\nnewdata = st_as_sf(data.frame(x = .1 * 0:20, y = 1), coords = c(\"x\", \"y\"))\nsk(data, newdata, 0, cov) # mu = 0\n#       [,1]\n#  [1,] 1.10\n#  [2,] 1.22\n#  [3,] 1.35\n#  [4,] 1.49\n#  [5,] 1.65\n#  [6,] 1.82\n#  [7,] 2.01\n#  [8,] 2.22\n#  [9,] 2.46\n# [10,] 2.71\n# [11,] 3.00\n# [12,] 2.71\n# [13,] 2.46\n# [14,] 2.22\n# [15,] 2.01\n# [16,] 1.82\n# [17,] 1.65\n# [18,] 1.49\n# [19,] 1.35\n# [20,] 1.22\n# [21,] 1.10\n\nPlotting them:\n\nnewdata = st_as_sf(data.frame(x = seq(-4, 6, by = .1), y = 1), coords = c(\"x\", \"y\"))\nZ = sk(data, newdata, 0, cov) # mu = 0\nplot(st_coordinates(newdata)[,1], Z, type = 'l', ylim = c(0, 3))\npoints(1, 3, col = 'red', pch = 16)\nabline(0, 0, col = 'blue', lty = 2)\n\n\n\n\nExample with zinc data:\n\ndata(meuse, package = \"sp\")\nmeuse = st_as_sf(meuse, coords = c(\"x\", \"y\"))\ndata(meuse.grid, package = \"sp\")\nmeuse[[1]] = log(meuse$zinc)\nlibrary(stars)\nmeuse.grid = st_as_stars(meuse.grid)\nmeuse.grid$sk = sk(meuse, \n                   st_as_sf(meuse.grid, as_points = TRUE, na.rm = FALSE), \n                   mu = mean(log(meuse$zinc)), \n                   cov = function(h) exp(-h/300))\n# mask:\nmeuse.grid$sk[ is.na(meuse.grid$dist) ] = NA\nplot(meuse.grid[\"sk\"], main = \"simple kriging\", axes = TRUE,\n    col = sf.colors(), breaks = \"equal\", reset = FALSE)\nplot(st_geometry(meuse), add = TRUE)"
  },
  {
    "objectID": "si3.html",
    "href": "si3.html",
    "title": "7  Unknown, constant mean",
    "section": "",
    "text": "\\[\n\\newcommand{\\E}{{\\rm E}}       % E expectation operator\n\\newcommand{\\Var}{{\\rm Var}}   % Var variance operator\n\\newcommand{\\Cov}{{\\rm Cov}}   % Cov covariance operator\n\\newcommand{\\Cor}{{\\rm Corr}}\n\\]\nSuppose the mean is constant, but not known. This is the most simple realistic scenario. We can estimate it from the data, taking into account their covariance (i.e., using weighted averaging):\n\\[\\hat{m} = ({\\bf 1}'V^{-1}{\\bf 1})^{-1} {\\bf 1}'V^{-1}Z\\] with \\({\\bf 1}\\) a conforming vector with ones, and substitute this mean in the SK prediction equations: BLUP/Ordinary kriging: \\[\\hat{Z}(s_0) = \\hat{m} + v'V^{-1} (Z-\\hat{m})\\] \\[\\sigma^2(s_0) = \\sigma^2_0 - v'V^{-1}v + Q\\] with \\(Q = (1 - {\\bf 1}'V^{-1}v)'({\\bf 1}'V^{-1}{\\bf 1})^{-1}(1 - {\\bf 1}'V^{-1}v)\\)\n\nStationarity 1\nGiven prediction location \\(s_0\\), and data locations \\(s_1\\) and \\(s_2\\), we need: \\(\\Var(Z(s_0))\\), \\(\\Var(Z(s_1))\\), \\(\\Var(Z(s_2))\\), \\(\\Cov(Z(s_0),Z(s_1))\\), \\(\\Cov(Z(s_0),Z(s_2))\\), \\(\\Cov(Z(s_1),Z(s_2))\\).\nHow to get these covariances? * given a single measurement \\(z(s_1)\\), we can not infer \\(\\Var(Z(s_1))\\) * given two measurements \\(z(s_1)\\) and \\(z(s_2)\\), we can never infer \\(\\Cov(Z(s_1),Z(s_2))\\) * geven a time series at \\(s_1\\) and \\(s_2\\), we can infer * \\(\\Cov(Z(s_1),Z(s_2))\\), but how to infer * \\(\\Cov(Z(s_0),Z(s_1))\\) and * \\(\\Cov(Z(s_0),Z(s_2))\\)?\nSolution: assume stationarity.\n\n\nStationarity 2\nStationarity of the * mean \\(\\E(Z(s_1)) = \\E(Z(s_2)) = ... = m\\) * variance \\(\\Var(Z(s_1)) = \\Var(Z(s_2)) = ... = \\sigma^2_0\\) * covariance \\(\\Cov(Z(s_1),Z(s_2)) = \\Cov(Z(s_3),Z(s_4))\\) if \\(s_1-s_2=s_3-s_4\\): distance/direction dependence\nSecond order stationarity: \\(\\Cov(Z(s),Z(s+h)) = C(h)\\)\nwhich implies: \\(\\Cov(Z(s),Z(s)) = \\Var(Z(s))= C(0)\\)\nThe function \\(C(h)\\) is the covariogram of the random function \\(Z(s)\\)\n\n\nFrom covariance to semivariance\nCovariance: \\(\\Cov(Z(s),Z(s+h)) = C(h) = \\E[(Z(s)-m)(Z(s+h)-m)]\\)\nSemivariance: \\(\\gamma(h) = \\frac{1}{2} \\E[(Z(s)-Z(s+h))^2]\\)\n\\(\\E[(Z(s)-Z(s+h))^2] = \\E[(Z(s))^2 + (Z(s+h))^2 -2Z(s)Z(s+h)]\\)\n\\(\\E[(Z(s)-Z(s+h))^2] = \\E[(Z(s))^2] + \\E[(Z(s+h))^2] - 2\\E[Z(s)Z(s+h)] = 2\\Var(Z(s)) - 2\\Cov(Z(s),Z(s+h)) = 2C(0)-2C(h)\\)\n\\(\\gamma(h) = C(0)-C(h)\\)\n\\(\\gamma(h)\\) is the semivariogram of \\(Z(s)\\).\n\n\nThe Variogram\n\nlibrary(sf)\n# Linking to GEOS 3.10.2, GDAL 3.4.3, PROJ 8.2.1; sf_use_s2() is TRUE\ndata(meuse, package = \"sp\")\nmeuse = st_as_sf(meuse, coords = c(\"x\", \"y\"))\nlibrary(gstat)\nv = variogram(log(zinc) ~ 1, meuse)\nv.fit = fit.variogram(v, vgm(1, \"Sph\", 900, 1))\nplot(v, v.fit)\n\n\n\n\n\n\nThe Variogram\n\nthe central tool to geostatistics\nlike a mean squares (variance) in analysis of variance, like a \\(t\\) to a \\(t\\)-test\nmeasures spatial correlation\nsubject to debate: it involves modelling\nsynonymous to semivariogram, but\nsemivariance is not synonymous to variance\n\n\n\nVariogram: how to compute\naverage squared differences: \\[\\hat{\\gamma}(\\tilde{h})=\\frac{1}{2N_h}\\sum_{i=1}^{N_h}(Z(s_i)-Z(s_i+h))^2 \\ \\\nh \\in \\tilde{h}\\] * divide by \\(2N_h\\): * if finite, \\(\\gamma(\\infty)=\\sigma^2\\) * semi variance * if data are not gridded, group \\(N_h\\) pairs \\(s_i,s_i+h\\) for which \\(h \\in \\tilde{h}\\), \\(\\tilde{h}=[h_1,h_2]\\) * choose about 10-25 distance intervals \\(\\tilde{h}\\), from length 0 to about on third of the area size * plot \\(\\gamma\\) against \\(\\tilde{h}\\) taken as the average value of all \\(h \\in \\tilde{h}\\)\n\n\nVariogram: terminology\n\nplot(v, v.fit)\n\n\n\nv.fit\n#   model  psill range\n# 1   Nug 0.0507     0\n# 2   Sph 0.5906   897\n\n\nvgm(psill = 0.6, model = \"Sph\", range = 900, nugget = 0.06)\n#   model psill range\n# 1   Nug  0.06     0\n# 2   Sph  0.60   900\n\nor simpler, with un-named arguments:\n\nvgm(0.6, \"Sph\", 900, 0.06)\n#   model psill range\n# 1   Nug  0.06     0\n# 2   Sph  0.60   900\n\n\n\nWhy prefer the variogram over the covariogram\nCovariance: \\(\\Cov(Z(s),Z(s+h)) = C(h) = \\E[(Z(s)-m)(Z(s+h)-m)]\\)\nSemivariance: \\(\\gamma(h) = \\frac{1}{2} \\E[(Z(s)-Z(s+h))^2]\\)\n\\(\\gamma(h)=C(0)-C(h)\\) * tradition * \\(C(h)\\) needs (an estimate of) \\(m\\), \\(\\gamma(h)\\) does not * \\(C(0)\\) may not exist (\\(\\infty\\)!), when \\(\\gamma(h)\\) does (e.g., Brownian motion)\n\n\nKnown, varying mean\nThis is nothing else then simple kriging, except that the mean is no longer constant; BLP/Simple kriging: \\[\\hat{Z}(s_0) = \\mu(s_0) + v'V^{-1} (Z-\\mu(s))\\] \\[\\sigma^2(s_0) = \\sigma^2_0 - v'V^{-1}v\\] with \\(\\mu(s) = (\\mu(s_1),\\mu(s_2),...,\\mu(s_n))'\\)."
  },
  {
    "objectID": "si4.html",
    "href": "si4.html",
    "title": "8  Unknown, varying mean",
    "section": "",
    "text": "\\[\n\\newcommand{\\E}{{\\rm E}}       % E expectation operator\n\\newcommand{\\Var}{{\\rm Var}}   % Var variance operator\n\\newcommand{\\Cov}{{\\rm Cov}}   % Cov covariance operator\n\\newcommand{\\Cor}{{\\rm Corr}}\n\\]\nFor this, we need to know how the mean varies. Suppose we model this as a linear regression model in \\(p\\) known predictors: \\[Z(s_i) = \\sum_{j=0}^p \\beta_j X_j(s_i) + e(s_i)\\] \\[Z(s) = \\sum_{j=0}^p \\beta_j X_j(s) + e(s) = X(s)\\beta + e(s)\\] with \\(X(s)\\) the matrix with predictors, and row \\(i\\) and column \\(j\\) containing \\(X_j(s_i)\\), and with \\(\\beta = (\\beta_0,...\\beta_p)\\). Usually, the first column of \\(X\\) contains zeroes in which case \\(\\beta_0\\) is an intercept.\nPredictor: \\[\\hat{Z}(s_0) = x(s_0)\\hat{\\beta} + v'V^{-1} (Z-X\\hat{\\beta}) \\] with \\(x(s_0) = (X_0(s_0),...,X_p(s_0))\\) and \\(\\hat{\\beta} = (X'V^{-1}X)^{-1} X'V^{-1}Z\\) it has prediction error variance \\[\\sigma^2(s_0) = \\sigma^2_0 - v'V^{-1}v + Q\\] with \\(Q = (x(s_0) - X'V^{-1}v)'(X'V^{-1}X)^{-1}(x(s_0) - X'V^{-1}v)\\)\nThis form is called external drift kriging, universal kriging or sometimes regression kriging.\nExample in meuse data set: log(zinc) depending on sqrt(meuse)\n\nlibrary(sf) # st_distance\n# Linking to GEOS 3.10.2, GDAL 3.4.3, PROJ 8.2.1; sf_use_s2() is TRUE\ncov = function(h) exp(-h)\n\ncalc_beta = function(X, V, z) {\n    XtVinv = t(solve(V, X))\n    solve(XtVinv %*% X, XtVinv %*% z)\n}\n\nuk = function(data, newdata, X, x0, cov, beta) {\n    V = cov(st_distance(data))\n    v = cov(st_distance(data, newdata))\n    z = data[[1]]\n    if (missing(beta))\n        beta = calc_beta(X, V, z)\n    mu = X %*% beta\n    x0 %*% beta + t(v) %*% solve(V, z - mu)\n}\n\n# prediction location at (0,1):\nnewdata = st_as_sf(data.frame(x = 0, y = 1), coords = c(\"x\", \"y\"))\n\n# observation location at (1,1), with attribute value (y) 3:\ndata = st_as_sf(data.frame(x = 1, y = 1, z = 3), coords = c(\"x\", \"y\"))\nx0 = matrix(1, 1, 1)\nX = x0\nuk(data, newdata, X, x0, cov)\n#      [,1]\n# [1,]    3\n\n# three observations location, with attribute values (y) 3,2,5:\ndata = st_as_sf(data.frame(x = c(1,2,3), y = c(1,1,1), z = c(3, 2, 5)),\n               coords = c(\"x\", \"y\"))\nnewdata = st_as_sf(data.frame(x = .1 * 0:20, y = 1), coords = c(\"x\", \"y\"))\nX = matrix(1,3,1)\nx0 = matrix(1, nrow(newdata), 1)\nuk(data, newdata, X, x0, cov) # mu = unknown\n#       [,1]\n#  [1,] 3.33\n#  [2,] 3.31\n#  [3,] 3.29\n#  [4,] 3.26\n#  [5,] 3.23\n#  [6,] 3.20\n#  [7,] 3.17\n#  [8,] 3.13\n#  [9,] 3.09\n# [10,] 3.05\n# [11,] 3.00\n# [12,] 2.94\n# [13,] 2.87\n# [14,] 2.79\n# [15,] 2.71\n# [16,] 2.62\n# [17,] 2.51\n# [18,] 2.40\n# [19,] 2.28\n# [20,] 2.15\n# [21,] 2.00\n\nPlotting them:\n\nnewdata = st_as_sf(data.frame(x = seq(-4,6,by=.1), y = 1), coords = c(\"x\", \"y\"))\nx0 = matrix(1, nrow(newdata), 1)\nz_hat = uk(data, newdata, X, x0, cov) # mu unknown\nplot(st_coordinates(newdata)[,1], z_hat, type='l', ylim = c(1,5))\npoints(st_coordinates(data)[,1], data$z, col='red', pch=16)\nbeta = calc_beta(X, cov(st_distance(data)), data[[1]])\nbeta\n#      [,1]\n# [1,] 3.52\nabline(beta, 0, col = 'blue', lty = 2)\n\n\n\n\n\nLinear trend:\n\nX = cbind(matrix(1,3,1), 1:3)\nX\n#      [,1] [,2]\n# [1,]    1    1\n# [2,]    1    2\n# [3,]    1    3\nxcoord = seq(-4,6,by=.1)\nnewdata = st_as_sf(data.frame(x = xcoord, y = 1), coords = c(\"x\", \"y\"))\nx0 = cbind(matrix(1, nrow(newdata), 1), xcoord)\nz_hat = uk(data, newdata, X, x0, cov) # mu unknown\nplot(st_coordinates(newdata)[,1], z_hat, type='l')\npoints(st_coordinates(data)[,1], data$z, col='red', pch=16)\nbeta = calc_beta(X, cov(st_distance(data)), data[[1]])\nbeta\n#      [,1]\n# [1,] 1.52\n# [2,] 1.00\nabline(beta, col = 'blue', lty = 2)\n\n\n\n\n\n\nWith Gaussian covariance:\n\ncov = function(h) exp(-(h^2))\nz_hat = uk(data, newdata, X, x0, cov) # mu unknown\nplot(st_coordinates(newdata)[,1], z_hat, type='l')\npoints(st_coordinates(data)[,1], data$z, col='red', pch=16)\nbeta = calc_beta(X, cov(st_distance(data)), data[[1]])\nbeta\n#      [,1]\n# [1,] 1.63\n# [2,] 1.00\nabline(beta, col = 'blue', lty = 2)\n\n\n\n\n\n\nEstimating spatial correlation under the UK model\nAs opposed to the ordinary kriging model, the universal kriging model needs knowledge of the mean vector in order to estimate the semivariance (or covariance) from the residual vector: \\[\\hat{e}(s) = Z(s) - X\\hat\\beta\\] but how to get \\(\\hat\\beta\\) without knowing \\(V\\)? This is a chicken-egg problem. The simplest, but not best, solution is to plug \\(\\hat{\\beta}_{OLS}\\) in, and from the \\(e_{OLS}(s)\\), estimate \\(V\\) (i.e., the variogram of \\(Z\\))\n\n\nSpatial Prediction\n… involves errors, uncertainties\n\n\nKriging varieties\n\nSimple kriging: \\(Z(s)=\\mu+e(s)\\), \\(\\mu\\) known\nOrdinary kriging: \\(Z(s)=m+e(s)\\), \\(m\\) unknown\nUniversal kriging: \\(Z(s)=X\\beta+e(s)\\), \\(\\beta\\) unknown\nSK: linear predictor \\(\\lambda'Z\\) with \\(\\lambda\\) such that \\(\\sigma^2(s_0) = \\E(Z(s_0)-\\lambda'Z)^2\\) is minimized\nOK: linear predictor \\(\\lambda'Z\\) with \\(\\lambda\\) such that it\n\nhas minimum variance \\(\\sigma^2(s_0) = \\E(Z(s_0)-\\lambda'Z)^2\\), and\nis unbiased \\(\\E(\\lambda'Z) = m\\)\nsecond constraint: \\(\\sum_{i=1}^n \\lambda_i = 1\\), weights sum to one.\n\nUK: \\[\\hat{Z}(s_0) = x(s_0)\\hat{\\beta} + v'V^{-1} (Z-X\\hat{\\beta}) \\] with \\(x(s_0) = (X_0(s_0),...,X_p(s_0))\\) and \\(\\hat{\\beta} = (X'V^{-1}X)^{-1} X'V^{-1}Z\\) \\[\\sigma^2(s_0) = \\sigma^2_0 - v'V^{-1}v + Q\\] with \\(Q = (x(s_0) - X'V^{-1}v)'(X'V^{-1}X)^{-1}(x(s_0) - X'V^{-1}v)\\)\nOK: fill in a column vector with ones for \\(X\\): \\(X=(1,1,...,1)'\\) and \\(X_0=1\\)\nSK: take out the trend/unknown mean\n\n\n\nUK and linear regression\nIf \\(Z\\) has no spatial correlation, all covariances are zero and \\(v=0\\) and \\(V=\\mbox{diag}(\\sigma^2)\\). This implies that \\[\\hat{Z}(s_0) = x(s_0)\\hat{\\beta} + v'V^{-1} (Z-X\\hat{\\beta}) \\] with \\(\\hat{\\beta} = (X'V^{-1}X)^{-1} X'V^{-1}Z\\) reduces to\n\\[\\hat{Z}(s_0) = x(s_0)\\hat{\\beta}\\] with \\(\\hat{\\beta} = (X'X)^{-1} X'Z\\), i.e., ordinary least squares regression.\nNote that\n\nunder this model the residual does not carry information, as it is white noise\nin spatial prediction, UK can not be worse than linear regression, as linear regression is a limiting case of a more general model."
  },
  {
    "objectID": "si5.html",
    "href": "si5.html",
    "title": "9  Local predictors, block kriging",
    "section": "",
    "text": "Global vs. local predictors\nIn many cases, instead of using all data, the number of observations used for prediction are limited to a selection of nearest observations, based on * number of observations or * distance to prediction location \\(s_0\\) * possibly, in addition, directions\nThe reason for this is usually either * statistical, allowing for a more flexible mean/trend structure * practical, if \\(n\\) gets large\n\n\nStatistical arguments for local prediction\n\nestimating \\(\\beta\\) locally instead of globally means that\n\n\\(\\beta\\) will adjust to local situations (less bias)\nit will be harder to estimate \\(\\beta\\) from less information, so (slightly?) larger prediction errors will result (larger variance)\n\\(X\\) needs to be non-singular in every neighbourhood\n\nsome authors claim that local trends are so adaptive, that one can ignore spatial correlation of the residual\nUsing local linear regression with weights that decay with distance is called geographically weighted regression, GWR\n\n\n\nPractical arguments for local prediction\n\nThe number of observations, \\(n\\) may become very large.\n\nlidar data, 3D chemical, satellite sensors, geotechnical, seismic, …\n\nComputing \\(V^{-1}v\\) is the expensive part; it is \\(O(N^2)\\) or \\(O(N^3)\\) as \\(V\\) is usually not of simple structure\nthere is a trade-off; for a global neighbourhood, the expensive part, factoring \\(V\\) needs only be done once, for a local neighbourhood for each unique neighbourhood (in practice: for each \\(s_0\\)).\nselecting local neighbourhoods also costs time; naive selection \\(O(n \\log n)\\) doesn’t scale well\ngstat uses quadtrees/octtrees, inspired by http://donar.umiacs.umd.edu/quadtree/index.html (defunct)\n\n\n\nPredicting block means\nInstead of predicting \\(Z(s_0)\\) for a “point” location, one might be interested at predicting the average of \\(Z(s)\\) over a block, \\(B_0\\), i.e. \\[Z(B_0) = \\frac{1}{|B_0|}\\int_{B_0} Z(u)du\\]\n\nThis can (naively) be done by predicting \\(Z\\) over a large number of points \\(s_0\\) inside \\(B_0\\), and averaging\nFor the prediction error, of \\(\\hat{Z}(B_0)\\), we then need the covariances between all point predictions\na more efficient way is to use block kriging, which does both at once\n\n\n\nReason why one wants block means\nExamples\n\nmining: we cannot mine point values\nsoil remediation: we cannot remediate points\nRS: we can match satellite image pixels\ndisaster management: we cannot evacuate points\nenvironment: legislation may be related to blocks\naccuracy: block means can be estimated with smaller errors than points"
  },
  {
    "objectID": "si6.html#cokriging",
    "href": "si6.html#cokriging",
    "title": "10  Cokriging, cross validation, conditional simulation",
    "section": "10.1 cokriging",
    "text": "10.1 cokriging\nCokriging sets the multivariate equivalent of kriging, which is, in terms of number of dependent variables, univariate. Kriging: \\[Z(s) = X(s)\\beta + e(s)\\] Cokriging: \\[Z_1(s) = X_1(s)\\beta_1 + e_1(s)\\] \\[Z_2(s) = X_2(s)\\beta_2 + e_2(s)\\] \\[Z_k(s) = X_k(s)\\beta_k + e_k(s)\\] with \\(V = \\Cov(e_1,e_2,...,e_k)\\)\nCases where this is useful: multiple spatial correlated variables such as\n\nchemical properties (auto-analyzers!)\nsediment composition\nelectromagnetic spectra (imagery/remote sensing)\necological data (abiotic factors; species abundances)\n(space-time data, with discrete time)\n\nTwo types of applications:\n\nundersampled case: secondary variables help prediction of a primary, because we have more samples of them (image?)\nequally sampled case: secondary variables don’t help prediction much, but we are interested in multivariate prediction, i.e. prediction error covariances.\n\n\nCokriging prediction\nCokriging prediction is not substantially different from kriging prediction, it is just a lot of book-keeping.\nHow to set up \\(Z(s)\\), \\(X\\), \\(\\beta\\), \\(e(s)\\), \\(x(s_0)\\), \\(v\\), \\(V\\)?\nMultivariable prediction involves the joint prediction of multiple, both spatially and cross-variable correlated variables. Consider \\(m\\) distinct variables, and let \\(\\{Z_i(s), X_i, \\beta^i, e_i(s), x_i(s_0), v_i, V_i\\}\\) correspond to \\(\\{Z(s), X, \\beta, e(s), x(s_0), v, V\\}\\) of the \\(i\\)-th variable. Next, let \\({\\bf Z}(s) = (Z_1(s)',...,Z_m(s)')'\\), \\({\\bf B}=({\\beta^1} ',...,{\\beta^m} ')'\\), \\({\\bf e}(s)=(e_1(s)',...,e_m(s)')'\\),\n\\[\n{\\bf X} =\n\\left[\n\\begin{array}{cccc}\nX_1 & 0 & ... & 0 \\\\\\\\\n0 & X_2 & ... & 0 \\\\\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\\n0 & 0 & ... & X_m \\\\\\\\\n\\end{array}\n\\right], \\\n{\\bf x}(s_0) =\n\\left[\n\\begin{array}{cccc}\nx_1(s_0) & 0 & ... & 0 \\\\\\\\\n0 & x_2(s_0) & ... & 0 \\\\\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\\n0 & 0 & ... & x_m(s_0) \\\\\\\\\n\\end{array}\n\\right]\n\\]\nwith \\(0\\) conforming zero matrices, and\n\\[{\\bf v} =\n\\left[\n\\begin{array}{cccc}\nv_{1,1} & v_{1,2} & ... & v_{1,m} \\\\\\\\\nv_{2,1} & v_{2,2} & ... & v_{2,m} \\\\\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\\nv_{m,1} & v_{m,2} & ... & v_{m,m} \\\\\\\\\n\\end{array}\n\\right], \\ \\\n{\\bf V} =\n\\left[\n\\begin{array}{cccc}\nV_{1,1} & V_{1,2} & ... & V_{1,m} \\\\\\\\\nV_{2,1} & V_{2,2} & ... & V_{2,m} \\\\\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\\nV_{m,1} & V_{m,2} & ... & V_{m,m} \\\\\\\\\n\\end{array}\n\\right]\n\\]\nwhere element \\(i\\) of \\(v_{k,l}\\) is \\(\\Cov(Z_k(s_i), Z_l(s_0))\\), and where element \\((i,j)\\) of \\(V_{k,l}\\) is \\(\\Cov(Z_k(s_i),Z_l(s_j))\\).\nThe multivariable prediction equations equal the previous UK equations and when all matrices are substituted by their multivariable forms (see also Ver Hoef and Cressie, Math.Geol., 1993), and when for \\(\\sigma^2_0\\), \\(\\Sigma\\) is substituted with \\(\\Cov(Z_i(s_0),Z_j(s_0))\\) in its \\((i,j)\\)-th element. Note that the prediction variance is now a prediction error covariance matrix.\n\n\nWhat is needed?\nThe main tool for estimating semivariances between different variables is the cross variogram, defined for collocated data as \\[\\gamma_{ij}(h) = \\mbox{E}[(Z_i(s)-Z_i(s+h))(Z_j(s)-Z_j(s+h))]\\] and for non-collocated data as \\[\\gamma_{ij}(h) = \\mbox{E}[(Z_i(s)-m_i)(Z_j(s+h)-m_j)]\\] with \\(m_i\\) and \\(m_j\\) the means of the respective variables. Sample cross variograms are the obvious sums over the available pairs or cross pairs, as in one of \\[\\hat{\\gamma}_{jk}(\\tilde{h})=\\frac{1}{N_h}\\sum_{i=1}^{N_h}(Z_j(s_i)-Z_j(s_i+h))(Z_k(s_i)-Z_k(s_i+h))\\] \\[\\hat{\\gamma}_{jk}(\\tilde{h})=\\frac{1}{N_h}\\sum_{i=1}^{N_h}(Z_j(s_i)-m_j)(Z_k(s_i+h)-m_k)\\]\n\n\nPermissible cross covariance functions\nTwo classes of permissible cross covariance (semivariance) functions are often used:\n\nintrinsic correlation (IC): \\[\\gamma_{jk}(h) = \\alpha_{jk} \\sqrt{\\gamma_{jj}(h)\\gamma_{kk}(h)}\\] parameters \\(\\alpha_{jk}\\) are correlation cofficients; very strict\nlinear model of coregionalization (LMC): \\[\\gamma_{jk}(h) = \\sum_{l=1}^p \\gamma_{jk,p}(h)\\] (e.g., nugget + spherical model), and \\[\\gamma_{jk,p}(h) = \\alpha_{jk,p} \\sqrt{\\gamma_{jj,p}(h)\\gamma_{kk,p}(h)}\\]\n\n\n\nHow to do this?\nAs multivariable analysis may involve numerous variables, we need to start organising the available information. For that reason, we collect all the observation data specifications in a gstat object, created by the function gstat. This function does nothing else than ordering (and actually, copying) information needed later in a single object. Consider the following definitions of four heavy metals:\n\nlibrary(sf)\n# Linking to GEOS 3.10.2, GDAL 3.4.3, PROJ 8.2.1; sf_use_s2() is TRUE\nlibrary(sp)\nlibrary(gstat)\ndata(meuse, package = \"sp\")\nmeuse = st_as_sf(meuse, coords = c(\"x\", \"y\"))\ng <- gstat(id = \"logCd\", formula = log(cadmium)~1, data = meuse)\ng <- gstat(g, \"logCu\", log(copper)~1, data = meuse)\ng <- gstat(g, \"logPb\", log(lead)~1, data = meuse)\ng <- gstat(g, \"logZn\", log(zinc)~1, data = meuse)\ng \n# data:\n# logCd : formula = log(cadmium)`~`1 ; data dim = 155 x 12\n# logCu : formula = log(copper)`~`1 ; data dim = 155 x 12\n# logPb : formula = log(lead)`~`1 ; data dim = 155 x 12\n# logZn : formula = log(zinc)`~`1 ; data dim = 155 x 12\nvm <- variogram(g)\nvm.fit <- fit.lmc(vm, g, vgm(1, \"Sph\", 800, 1))\nplot(vm, vm.fit)\n\n\n\n\n\n\nKriging predictions and errors – how good are they?\nCross validation can be used to assess the quality of any interpolation, including kriging. We split the data set in \\(n\\) parts (folds). For each part, we\n\nleave out the observations of this fold\nuse the observations of all other folds to predict the values at the locations of this fold\ncompare the predictions with the observations This is called \\(n\\)-fold cross validation. If \\(n\\) equals the number of observation, it is called leave-one-out cross validation (LOOCV).\n\n\n\nCross validation: what does it yield?\n\nresiduals \\(r(s_i) = z(s_i) -\\hat{z}(s_i)\\) – histograms, maps, summary statistics\nmean residual should be near zero\nmean square residual \\(\\sum r(s_i)^2\\) should be as small as possible\n\nIn case the interpolation method yields a prediction error we can compute z-scores: \\(r(s_i)/\\sigma(s_i)\\)\nThe z-score allows the validation of the kriging error, as the z-score should have mean close to zero and variance close to 1. If the variance of the z-score is larger (smaller) than 1, the kriging standard error is underestimating (overestimating) the true interpolation error, on average.\n\n\nKriging errors – what do they mean?\nSuppose legislation prescribes remediation in case zinc exceeds 500 ppm. Where does the zinc level exceed 500 ppm?\n\nwe can compare the map of the predictions with 500. However:\n\n\\(\\hat{z}(s_0)\\) does not equal \\(z(s_0)\\):\n\\(\\hat{z}(s_0)\\) is more smooth than \\(z(s_0)\\)\n\\(\\hat{z}(s_0)\\) is closer to the mean than \\(z(s_0)\\)\nsmoothing effect is stronger if spatial correlation is small or nugget effect is relatively large\n\nalternatively we can assume that the true (unknown) value follows a probability distribution, with mean \\(\\hat{z}(s_0)\\) and standard error \\(\\sigma(s_0)\\).\nthis latter approach acknowledges that \\(\\sigma(s_0)\\) is useful as a measure of interpolation accuracy\n\n\n\nConditional probability\n\nwe can use e.g. the normal distribution (on the log-scale?) to assess the conditional probability \\(\\Pr(Z(s_0) > 500 | z(s_1),...,z(s_n))\\)\nthe additional assumption underlying this is multivariate normality: in addition to having stationary mean and covariance, the field \\(Z\\) is now assumed to follow a stationary, multivariate normal distribution. This means that any single \\(Z(s_i)\\) follows a normal distribution, and any pair \\(Z(s_i), Z(s_j)\\) follows a bivariate normal distribution, with known variances and covariance.\n\nHow?\n\nv = variogram(log(zinc)~1, meuse)\nv.fit = fit.variogram(v, vgm(1, \"Sph\", 900, 1))\ndata(meuse.grid, package = \"sp\")\nlibrary(stars)\n# Loading required package: abind\nmeuse.grid = st_as_stars(meuse.grid)\nout = krige(log(zinc)~1, meuse, meuse.grid, v.fit)\n# [using ordinary kriging]\nout$p500 = 1 - pnorm(log(500), out$var1.pred, sqrt(out$var1.var))\nplot(out[\"p500\"], col = sf.colors(), breaks = \"equal\")\n\n\n\n\n\n\nIndicator kriging\nAnother approach to estimating probabilities of exceedance is to consider the indicator function, which is 1 if a value exceeds the threshold and 0 otherwise:\n\nmean(meuse$zinc)\n# [1] 470\nmean(meuse$zinc < 500)\n# [1] 0.632\n\n\nv = variogram(I(zinc > 500)~1, meuse)\nv_I.fit = fit.variogram(v, vgm(.2, \"Sph\", 900, .02))\nplot(v, v_I.fit)\n\n\n\nout$I500 = krige(I(zinc > 500)~1, meuse, meuse.grid, v_I.fit)$var1.pred\n# [using ordinary kriging]\nout[\"I500\"] # summarizes\n# stars object with 2 dimensions and 1 attribute\n# attribute(s):\n#          Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's\n# I500  -0.0876  0.0546  0.193 0.296   0.525 1.03 5009\n# dimension(s):\n#   from  to offset delta x/y\n# x    1  78 178440    40 [x]\n# y    1 104 333760   -40 [y]\nplot(merge(out[c(\"p500\", \"I500\")]), col = bpy.colors(), breaks = \"equal\")\n\n\n\n\nThis second approach:\n\nignores the kriging variance\ngenerates probabilities outside the interval \\([0, 1]\\) (to be corrected?)\nignores information whether observations are close to the threshold, or far away from it (they are reduced to 1/0 variable before interpolating)\ndoes not assume multivariate normality\ndoes not distinguish between estimated probabilities and (true) probabilities\nlends itself to the interpolation of categorical (nominal, ordinal) variables\ncokriging is sometimes used for interpolating several indicator variables (multiple categories, or multiple thresholds)\n\n\n\nConditional simulation\n\nv = variogram(log(zinc)~1, meuse)\nv.fit = fit.variogram(v, vgm(1, \"Sph\", 900, 1))\nout = krige(log(zinc)~1, meuse, meuse.grid, v.fit, nmax = 20, nsim = 9)\n# drawing 9 GLS realisations of beta...\n# [using conditional Gaussian simulation]\nout = split(out)\nout$kriging = krige(log(zinc)~1, meuse, meuse.grid, v.fit)$var1.pred\n# [using ordinary kriging]\nplot(merge(out), col = sf.colors(), breaks = \"equal\")\n\n\n\n\n\nsf = st_as_sf(out, as_points = TRUE)\nv_kr = variogram(kriging~1, sf)\nv_cs = variogram(sim1~1, sf)\nplot(v_kr)\n\n\n\nplot(v_cs)\n\n\n\nplot(v)\n\n\n\n\n\nconditional simulation creates multiple realisations of the field \\(Z(s)\\) that\n\nfollow the data points (pattern, reproduce observations like kriging)\nhave a variability equal to Z(s)\nhave a spatial correlation (variogram) equal to that of Z(s)\n\nas opposed to kriging, the resulting images are not smooth\nthis is useful e.g. if images are needed as input to subsequent processing / modelling, where the statistical properties of Z(s) need to be retained (e.g. simulating rainfall fields as input to rainfall-runoff models, to predict the likelihood of flooding / extreme water levels)"
  },
  {
    "objectID": "de1.html#deterministic-spatial-dynamic-models",
    "href": "de1.html#deterministic-spatial-dynamic-models",
    "title": "11  Differential equations",
    "section": "11.1 Deterministic spatial dynamic models",
    "text": "11.1 Deterministic spatial dynamic models\nDeterministic models are based on the assumption that the processes governing change are known. Given that knowledge, we need\n\nthe state of a system (initial conditions), and\nthe characteristics at boundaries of the system (boundary conditions): what are the sources and sinks, when does what escape or enter the modelled area.\n\nfor a (perfect) prediction of the changes over time, and in space. Examples of output of such models include all prediction models, weather models, and atmospheric composition models\nLet us look at an example: air quality (fine particles, PM10).\n\nModel domain\nFor a model, we need a model domain, which is the spatial area and temporal period over which we will define processes and conditions. This could be e.g. Western Europe, 1980-2010, or NRW, 2000-2005, or the area not further than 100 m away from the crossing Weseler Strasse-Bonhoeffer Strasse, Jan 8, 2008, 0:00-24:00. It should be exactly quantifiable/traceable.\n\n\nInitial conditions\nThe initial conditions usually describe the variable of interest (the concentration field) at the highest possible level of detail. In our case this is the PM10 concentration field at the start of the modelling period.\nAs this is a continuous field, we need some way to describe this and usually the spatial domain is discretized into model usually square, rectuangular or triangular model elements.\nThis discretization should match the level of detail\n\nat which we know initial conditions and\nat which we want to model features.\n\nAs an example: if we want to quantify the effect of individual car fumes, spatial elements of 10 cm–1 m may work; if we want to describe the effect of a complete streets something of 10m–100m seems more appropriate. Smaller elements and time steps mean more memory and CPU time requirements.\n\n\nInitial conditions-2\nIf we don’t know initial conditions exactly, we may put the starting point of the modelling domain further back in the past, and hope that the effect of approximation will damp out as we model. (This assumes we get the boundary conditions and processes right.)\n\n\nBoundary conditions\nPM10 comes and goes. Sources are (i) emissions inside the model domain (cars, households, industry), and (ii) material that enters the model domain by movement of air bodies, but emitted elsewhere. We need these source terms (points, lines or fields) in space, and over time.\nSinks are mostly air that moves out of the model domain, and wash out (rain), dry deposition (your grandmother’s white sheets turning black), and … inhalation. These terms are also needed, quantitatively.\n\n\nProcesses\nParticles move mostly for two or three reasons: by large-scale movement of air (wind), by medium/small-scale movement of air (turbulence, dispersion) and by themselves (diffusion; think Brownian motian of a single particle in a gas).\nAs an example, take a look at the LOTOS-EUROS model documentation.\nAs you can read in the model formulation and domain, the model uses external modelling results (interpolation or mechanistic modelling) to get the atmospheric driving forces (height mixing layer, wind fields), e.g. from FUB and ECMWF.\nBasically, the model code\n\nreads a lot of initial and boundary data,\nsolves the differential equations and\nwrites out everything that is of interest, such as the space-time concentration fields.\n\n\n\nSolving differential equations\nThe partial differential equation solved in LOTOS_EUROS is the continuity equation \\[\n\\frac{\\delta C}{\\delta t} +\nU\\frac{\\delta C}{\\delta x} +\nV\\frac{\\delta C}{\\delta y} +\nW\\frac{\\delta C}{\\delta z} \\] \\[\n= \\frac{\\delta}{\\delta x}(K_h \\frac{\\delta C}{\\delta x})  +\n\\frac{\\delta}{\\delta y}(K_h \\frac{\\delta C}{\\delta y})  +\n\\frac{\\delta}{\\delta z}(K_z \\frac{\\delta C}{\\delta z})  +E+R+Q-D-W\n\\] with (mostly quoted from the reference guide, page 6/7:\n\n\\(t,x,y,z\\) time and the three space dimensions\n\\(C\\) concentration of the pollutant (dynamic field)\n\\(U,V,W\\) the large scale wind components in respectively west-east direction, in south-north direction and in vertical direction\n\\(K_h,K_z\\) the horizontal and vertical turbulent diffusion coefficients\n\\(E\\) the entrainment or detrainment due to variations in layer height.\n\\(R\\) the amount of material produced or destroyed as a result of chemistry.\n\\(Q\\) is the contribution by emissions,\n\\(D\\) and \\(W\\) loss terms due to processes of dry and wet deposition respectively\n\nTo solve this equation, it needs to be discretized in space and time. For this particular model, the spatial grid size is 0.5 degree longitude \\(\\times\\) 0.25 degree latitude (meaning that grid cells do not have constant area), and time step is 1h.\n\n\nSolving PDE’s\nThe simples method to solve PDE’s by discretization is, finite difference, uses a regular mesh size, \\(\\Delta x\\). The solution is computed at location \\(j \\Delta x\\), with \\(j=1,...,N\\).\nIn one dimension the first derivative uses one of the three approximations:\n\nbackward: \\[\\frac{\\delta u}{\\delta x}(j \\Delta x) \\approx \\frac{u_j-u_{j-1}}{\\Delta x} \\]\nforward: \\[\\frac{\\delta u}{\\delta x}(j \\Delta x) \\approx \\frac{u_{j+1}-u_{j}}{\\Delta x} \\]\ncentered: \\[\\frac{\\delta u}{\\delta x}(j \\Delta x) \\approx \\frac{u_{j+1}-u_{j-1}}{2\\Delta x} \\] and for the second order derivative (centered): \\[\\frac{\\delta^2 u}{\\delta x^2}(j \\Delta x) \\approx \\frac{u_{j+1}-2u_j+u_{j-1}}{(\\Delta x)^2} \\]\n\n\n\nDiffusion equations, 1-D\nDiffusion happens in space-time. Using a mesh in space-time, we can write \\(u(j \\Delta x, n \\Delta t) \\approx u^n_j\\) with \\(n\\) a superscript, not power.\nWe can approximate the PDE \\[\\frac{\\delta u}{\\delta t} = \\frac{\\delta^2 u}{\\delta x^2}, \\ \\mbox{with} \\ u(x,0)=\\phi(x) \\]\nby finite difference, in time: \\[\\frac{\\delta u}{\\delta t}(j \\Delta x, n \\Delta t) \\approx \\frac{u_{j}^{n+1}-u_{j}^n}{\\Delta t}\\] and space: \\[\\frac{\\delta u}{\\delta x}(j \\Delta x, n \\Delta t) \\approx \\frac{u_{j+1}^{n}-u_j^n}{\\Delta x}\\]\nUsing forward difference for \\(t\\) and centered for \\(x\\), combining both, the corresponding finite difference equation that approximates the PDE it is: \\[\\frac{u_j^{n+1}-u_j^n}{\\Delta t} = \\frac{u^n_{j+1}-2u_j^n+u_{j-1}^n}{(\\Delta x)^2}.\\]\n\n\nForward/backward, explicit/implicit\nSolving \\[\\frac{u_j^{n+1}-u_j^n}{\\Delta t} = \\frac{u^n_{j+1}-2u_j^n+u_{j-1}^n}{(\\Delta x)^2}.\\] is trivial, as \\(n+1\\) is only in the LHS. This means that for each \\(x\\) we can solve the equation explicitly, where we start is not important. They require, for stable solutions, that \\(\\frac{\\Delta t}{(\\Delta x)^2} \\le \\frac{1}{2}\\). See examples.\nIf the equation were instead \\[\\frac{u_j^{n+1}-u_j^n}{\\Delta t} = \\frac{u^{n+1}_{j+1}-2u_j^{n+1}+u_{j-1}^{n+1}}{(\\Delta x)^2}.\\] then we have the unknown \\(u^{n+1}\\) both left and right of the equal sign. This requires the solution of a (sparse) set of coupled linear equations, and this solution is called implicit. It pays off: the solutions are stable, and larger time steps can be chosen (provided of course, that change is close to linear over the time step).\n\n\nCalibrating deterministic models\nModels based on partial differential equations have parameters; think of diffusion parameters, source and sink terms (boundary conditions), and initial conditions. These need to be filled in, somehow.\nGiven that observation data on the model outcome are available, one way to fill these in is to search for values such that the model predictions best fit the data. We have seen methods for this; there is a long list of further, possibly more advanced or efficient methods for finding optimal parameter values, both in a deterministic (optimum) sense, and in a stochastic (distribution) sense.\nAlso, choosing optimality criterium (least squares? least absolute differences? combined criteria over multiple variables?)\n\n\nDifficulties in calibration\nProblems that may occur with calibrating models are numerous.\nOne problem may be that the parameter we tune (optimize) is not constant over space or time, but varies. This means that there instead of one single value, there may be numerous. Their number may outnumber the observations, and in that case there is little hope in finding realistic values.\nAnother problem is that we may tune a parameter and get a better fit, but that in reality we turned the wrong button, meaning we get a better fit for the wrong reason. This may have disasterous effects when using this optimized model in a prediction setting (such as future forecasting, or scenario evaluation).\nAutomatic codes exist (e.g. PEST, or ‘’optim’’ in R) that optimize models, irrespective what the model is or does.\n\n\nMore difficulties\nDeterministic models use a temporal and spatial discretization. This is a balance between CPU and memory costs, and the ability to fill the discrete elements sensibly. Processes need to be lumped, meaning that they cannot be taken into account because of the grid cell size (think of convection above a forest, or a thunder storm, when grid cell size is 50 km, and/or time step a day). Choosing a finer resolution, the parameters, processes, boundary and initial values need to be filled in with much more resolution (precision), and need disaggregation – e.g. a country total emission may need to be assigned to 1 km \\(\\times\\) 1 km grid cells.\n\n\nDynamic parameter updating schemes\nA probabilistic setting of a deterministic model is that of the Kalman filter. This algorithm assumes measurements are a combination of a true state and a measurement noise. Each component has its particular error structure, expressed by a mean and covariance matrix.\nFor each new time step, the model predicts a new state, the observations are compared to that new state, and the model errors are used to adjust (improve) the model before predicting the next step.\nKalman filters are used a lot to optimize deterministic models, and come nowadays in many flavours, e.g. depending on whether the model is linear or not, and whether it is used forward in time, or in real-time.\nParticle filters do this in a stochastic setting.\n\n\nSimplified difference equation-type models\nOften, the differential equations are simplified very crudely to the state where only mass is preserved, but the solution no longer approximates the true differential equation. Think of simple bucket-type models in hydrology, where water bodies are buckets and soils are like sponges: a soil grid cell drains always with an exponential decrease; a soil and water body grid cells drain instantly when their maximum capacity is exceeded, with the amount it is exceeded.\nDespite the simplifications, these models can be more useful than attempts to solve the full differential equation, because their data demand can be more realistically met.\n\n\nCalibration vs. validation of models\nValidation of models involves assessing their validity, or value, and is a principally different activity from calibration. Where calibration is an optimization, validation is an assessment. The validation may be quantitative (“the mean square prediction error of the model is 0.75”) or qualitative (“the model is fit for its purpose”).\nValidation may also involve model comparison: the comparison of different models having different model structure, and can in that sense be seen as an abstraction over calibration:\n\ncalibration compares the models that have identical structure, but have different parameter values\nvalidation compares models that differ in structure\n\nModel comparison, in th course, would e.g. be:\n\ncomparison of AR(1), AR(2), …, AR(n) to describe a particular time series\ncomparison of ordinary kriging, universal kriging with predictor \\(X_1\\), and universal kriging with predictor \\(X_1\\) and \\(X_2\\) for a given interpolation problem.\n\nIn this case, the models compared are nested (i.e., AR(1) is a special case of AR(2), etc), which may simplify the problem to testing the significance of the extension (is the additional parameter significantly different from zero?) If they are not nested, comparison becomes harder, conceptually.\nA famous model comparison on the other end of the extreme is that of the coupled model intercomparison project, CMIP, e.g. CMIP; it involves large, international groups of climate scientists who agree, prior to the experiment, which models to compare."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Camara, Gilberto, Max J Egenhofer, Karine Ferreira, Pedro Andrade,\nGilberto Queiroz, Alber Sanchez, Jim Jones, and Lubia Vinhas. 2014.\n“Fields as a Generic Data Type for Big Spatial Data.” In\nInternational Conference on Geographic Information Science,\n159–72. Springer. https://link.springer.com/content/pdf/10.1007/978-3-319-11593-1_11.pdf.\n\n\nChatfield, Chris. 2003. The Analysis of Time Series: An\nIntroduction. Chapman; hall/CRC.\n\n\nEykhoff, Pieter. 1974. System Identification. Vol. 14. Wiley\nLondon.\n\n\nPebesma, Edzer, and Roger Bivand. 2022. Spatial Data Science, with\nApplications in R. online. https://r-spatial.org/book/.\n\n\nTobler, W. R. 1970. “A Computer Movie Simulating Urban Growth in\nthe Detroit Region.” Economic Geography 46: 234–40. https://doi.org/10.2307/143141."
  }
]