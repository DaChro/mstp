---
title: "Lecture 7"
subtitle: "Simple and Ordinary Kriging"
date: "2024-11-26"

include-in-header:
  - text: |
      <style>
      .reveal .slide-logo {
        max-height: unset;
        height: 70px;
      }
      </style>
format: revealjs
editor: visual
---

## How can correlation help prediction?

\newcommand{\E}{{\rm E}}       
\newcommand{\Var}{{\rm Var}}   
\newcommand{\Cov}{{\rm Cov}}   
\newcommand{\Cor}{{\rm Corr}}

Problem:

![](./data/interp.png)

Questions: Given observation $z(s_1)$, how to predict $z(s_0)$?

-   What is the best predicted value at $s_0$, $\hat{z}(s_0)$?
-   How can we compute a measure of error for $\hat{z}(s_0)-z(s_0)$?
-   Can we compute e.g.\~95% prediction intervals for the unknown $z(s_0)$?

## How can correlation help prediction?

Problem:

![](./data/interp.png)

Obviously, given *only* $z(s_1)$, the best predictor for $z(s_0)$ is $\hat{z}(s_0)=z(s_1)$.

But what is the error variance, i.e. $\mbox{Var}(\hat{z}(s_0)-z(s_0))$?

## How can correlation help prediction?

Estimation error:

Let both $z(s_1)$ and $z(s_0)$ come from a field that has constant variance and a constant mean: $\mbox{E}(z(s_0)) = \mbox{E}(z(s_1))=m$

Then, $$\mbox{Var}(\hat{z}(s_0)-z(s_0)) = \mbox{Var}(z(s_1)-z(s_0))$$

. . . 

As both have the same mean, this can be written as $$\mbox{E}(\hat{z}(s_0)-z(s_0))^2 = \mbox{Var}(z(s_1)) + \mbox{Var}(z(s_0)) - 2\mbox{Cov}(z(s_1),z(s_0))$$

. . . 


## How can correlation help prediction?

Next Problems...

![](./data/interp2.png)

## How can correlation help prediction?

Next Problems...

![](./data/interp3.png)

## What is geostatistical interpolation

Geostatistical interpolation (kriging) uses linear predictors $$\hat{z}(s_0) = \sum_{i=1}^n \lambda_i z(s_i)$$ with weights chosen such that

-   the interpolated value is unbiased: $\mbox{E}(\hat{z}(s_0)-z(s_0))=0$ and
-   has mininum variance: $\mbox{Var}(\hat{z}(s_0)-z(s_0))$ is at minimum.

All that is needed is variances and correlations.

## Simple Kriging

-   First, we assume that the mean of the observed process is known and constant

-   This case of prediction is called simple kriging

## Simple Kriging - procedure 


:::{.incremental}
-   compute covariance matrix $V$ containing the covariances between all pairs of the known points $Z(s_i)$ and $Z(s_j)$
-   compute covariance vector $v$ between all known points $Z(s_i)$ and the prediction point $Z(s_0)$
-   then the vector of the kriging weights $\lambda$ is computed as:
$$\lambda = V^{-1}v$$
-   the covariances can be calculated using the covariance function optained from the *variogram* (see last lecture) and the distances between the points
:::

## Simple Kriging - procedure
:::{.incremental}
-   simple kriging assumes the mean $\mu$ to be known and constant

-   so we can compute the difference of $\hat{Z}(s_0)$ to $\mu$ using the differences of all points to $\mu$: $\Delta = Z(s_i) - \mu$

-   given the vector of all these differences $\Delta$ and the vector of all weights $\lambda$ the predicted value is $$\hat{Z}(s_0) = \mu + \lambda'\Delta$$

-   so we compute the scalar product of the vector of all differences to the mean $\Delta$ and the weight vector $\lambda$ and add the mean $\mu$
:::

## Simple Kriging - explanation

But how did we get this equation?? $$\lambda = V^{-1}v$$

## Simple Kriging - explanation

recall from slide 7

Geostatistical interpolation (kriging) uses linear predictors $$\hat{z}(s_0) = \sum_{i=1}^n \lambda_i z(s_i)$$ with weights chosen such that

-   the interpolated value is unbiased: $\mbox{E}(\hat{z}(s_0)-z(s_0))=0$ and
-   has mininum variance: $\mbox{Var}(\hat{z}(s_0)-z(s_0))$ is at minimum.

All that is needed is variances and correlations.

## The quadratic form

We will not consider single random variables, but rather large collections of them. In fact, we will consider each observation $z(s_i)$ as a realisation (outcome) of a random variable $Z(s_i)$, and consider the $Z$ variable at all other locations also as separate random variables, say $Z(s_0)$ for any $s_0$ in the domain of interest.

Let $Z = [Z(s_1)\ Z(s_2)\ ...\ Z(s_n)]'$ then ${\rm Var}(Z)=V$ is the covariance matrix of vector $Z$, with $i,j$-th element ${\rm Cov}(Z(s_i),Z(s_j))$, implying it has variances on the diagonal.

Then, it is easy to show that for non-random weights $\lambda = [\lambda_1 ... \lambda_n]'$ the quadratic form $\lambda'Z = \sum_{i=1}^n \lambda_i Z(s_i)$ has variance $$ {\rm Var}(\lambda'Z) = \lambda' {\rm Var}(Z) \lambda = 
\sum_{i=1}^n \sum_{j=1}^n \lambda_i \lambda_j {\rm Cov}(Z(s_i),Z(s_j)) = \lambda'V\lambda$$

## Simple Kriging - explanation

Why do we need this?

When we predict (interpolate), we're forming linear combinations, $\sum_{i=1}^n \lambda_i Z(s_i)$, and want to know the variance of $\sum_{i=1}^n
\lambda_i Z(s_i) - Z(s_0)$, the interpolation error variance. Only then can we find weights such that it is minimum.

What is the scalar ${\rm Var}(\sum_{i=1}^n \lambda_i Z(s_i)-Z(s_0))$? Write as $${\rm Var}(\lambda'Z - Z(s_0)) = {\rm Var}(\lambda'Z) + {\rm Var}(Z(s_0)) - 2{\rm Cov}(\lambda'Z,Z(s_0))$$ $$=\lambda'V\lambda + \sigma_0^2 + \sum_{i=1}^n \lambda_i {\rm Cov}(Z(s_i),Z(s_0)) $$ with $\sigma_0^2 = {\rm Var}(Z(s_0))$

## Simple Kriging - explanation

So, we need variances of all $Z(s_i)$, including for all $s_0$, and all covariances between pairs $Z(s_i)$ and $Z(s_j)$, including all $s_0$.

## Simple Kriging - explanation

Suppose we know all that:

-   Kriging: find weights $\lambda$ such that ${\rm Var}(Z(s_0)-\hat{Z}(s_0))=
    {\rm Var}(Z(s_0)-\sum_{i=1}^n\lambda_i Z(s_i))$ is minimized, and we have the best (minimum variance) linear predictor.

-   Best linear prediction weights: Let $V={\rm Var}(Z)\ \ (n\times n)$ and $v={\rm Cov}(Z(s_0),Z)\ \ (n\times 1)$, and scalar ${\rm Var}(Z(s_0)) = \sigma^2_0$.

-   Expected squared prediction error: ${\rm E}(Z(s_0)-\hat{Z}(s_0))^2$= $\sigma^2(s_0)$

-   we need to minimize $\sigma^2(s_0)$ (Why?)

## Simple Kriging - explanation

-   Since the *mean* of the prediction error is 0 (unbiasedness condition) ${\rm E}(Z(s_0)-\hat{Z}(s_0))^2$ equals the *variance* of the prediction error ${\rm Var}(Z(s_0)-\hat{Z}(s_0))$

-   recall the following formula for variance, try to think of our prediction error as X and the mean of the prediction error as $m$:

$$\sigma^2(X) = {\rm E}(X-m)^2$$

if $m = 0$ then

$$\sigma^2(X) = {\rm E}(X)^2$$

-   Expected squared prediction error ${\rm E}(Z(s_0)-\hat{Z}(s_0))^2 = \sigma^2(s_0)$
-   we need to minimize $\sigma^2(s_0)$

## Simple Kriging - explanation

-   Replace $Z$ with $Z-\mu$ (or assume $\mu=0$) $$\sigma^2(s_0) = {\rm E}(Z(s_0)-\lambda ' Z)^2 =
    {\rm E}(Z(s_0))^2 - 2 \lambda '{\rm E}(Z(s_0) Z)+\lambda'{\rm E}(Z Z')\lambda $$ $$ = {\rm Var}(Z(s_0)) - 2 \lambda'{\rm Cov}(Z(s_0),Z) + \lambda'{\rm Var}(Z)\lambda
    = \sigma^2_0 - 2 \lambda'v + \lambda'V\lambda $$

-   Choose $\lambda$ such that $\frac{\delta \sigma^2(s_0)}{\delta\lambda} = -2 v' + 2\lambda'V = 0$

-   $\lambda' = v' V^{-1}$

BLP/Simple kriging:

1.  $\hat{Z}(s_0) = \mu + v'V^{-1} (Z-\mu)$
2.  $\sigma^2(s_0) = \sigma^2_0 - v'V^{-1}v$

## Simple Kriging - summary

-   $\lambda' = v' V^{-1}$ is the same as $\lambda = V^{-1}v$

-   so that is how we get the equation on slide 9

![](./data/simplekriging.jpg)

-   to predict unknown values $\hat{Z}(s_0)$ we interpolate their differences to the mean $\mu$ (using the differences to $\mu$ of the known points and the corresponding weights) and then add $\mu$ again


## Simple Kriging: Step-by-step example

**Situation:**
```{r, fig.width=3, fig.height=3,echo=F,fig.align='center'}
par(mar=c(3,2,0,1))
xy = data.frame(x=c(1,4,3,0), y=c(1,2,5,3), z=c(4,5,3,2))
plot(xy[c("x","y")],xlim=c(0,5),ylim=c(0,5))
for(i in 1:5) {
  abline(v=i,col="gray80")
  abline(h=i,col="gray80")
}
points(xy[c("x","y")],col="black",pch=19)
#labels=paste0("z=",as.character(xy$z))
labels = c(
  expression(paste("z"[1], "=4", sep="")),
  expression(paste("z"[2], "=5",sep="")),
  expression(paste("z"[3], "=3", sep="")),
  expression(paste("z"[4], "=2", sep=""))
  )
text(x = xy$x+0.5, y=xy$y, labels,cex=0.9)

xy0 = data.frame(x=1,y=2)
points(xy0,col="red",pch=16)
text(x = xy0$x+0.5, y=xy0$y, expression(paste("z"[0], "=?", sep="")),cex=0.9,col="red")
```

## Simple Kriging: Step-by-step example

Suppose we already know the (constant) mean $\mu = 4$ and the theoretical covariance function $$
\textrm{cov}(h) = 
\begin{cases}
1 & h = 0 \\
0.9 ~ \textrm{exp}\left(\frac{-h}{3} \right)& h > 0
\end{cases}
$$ which is the exponential model with nugget $0.1$, psill $0.9$, and range $3$. We want to predict a value for $(x,y) = (1,2)$.

## Simple Kriging: Step-by-step example

**Step 1**: Compute pairwise distances of known observations:

$$
D_{ij} = \sqrt{(x_i-x_j)^2 + (y_i-y_j)^2}
$$

$$
D_{ij} \approx
\begin{pmatrix}
0.00&3.16&4.47&2.24 \\
3.16&0.00&3.16&4.12 \\
4.47&3.16&0.00&3.61 \\
2.24&4.12&3.61&0.00 \\
\end{pmatrix}
$$

## Simple Kriging: Step-by-step example

**Step 2**: Compute pairwise covariance matrix of known observations:

$$
V_{ij} = \textrm{cov}(D_{ij})
$$

$$
V_{ij} \approx
\begin{pmatrix}
1.00&0.31&0.20&0.43 \\
0.31&1.00&0.31&0.23 \\
0.20&0.31&1.00&0.27 \\
0.43&0.23&0.27&1.00 \\
\end{pmatrix}
$$

## Simple Kriging: Step-by-step example

**Step 3**: Compute distances and covariances between predication location and observation locations:

$$
d_i = \sqrt{(x_i-x_0)^2 + (y_i-y_0)^2} ~~~~~~ v_i = \textrm{cov}(d_i) 
$$

$$
d_i \approx (1, 3, 3.61, 1.41)'  ~~~~~~ v_i \approx (0.64, 0.33, 0.27, 0.56)' 
$$

## Simple Kriging: Step-by-step example

**Step 4**: Solve the Kriging system:

$$
\lambda = V^{-1}v ~~~~~ \Rightarrow ~~~~~ \lambda \approx (0.46,0.09,0.06,0.33)'
$$

**Step 5**: Predict the value $z_0$:

$$
\begin{aligned}
\hat{z_0} &= \mu + \lambda' (z - \mu) \\
& \approx 4 + (0.46,0.09,0.06,0.33) \cdot (0,  1, -1, -2)' \\
&\approx 3.38
\end {aligned} 
$$

**Step 6**: Compute the Kriging variance:

$$ 
\begin {aligned} 
\hat{\sigma}^2_0 &= \textrm{cov}(0) - \lambda' v \\
& \approx 1 - (0.46,0.09,0.06,0.33) \cdot (0.64,0.33,0.27,0.56)' \\
& \approx 0.47
\end {aligned} 
$$

## Known, varying mean

This is nothing else then simple kriging, except that the mean is no longer constant; BLP/Simple kriging: $$\hat{Z}(s_0) = \mu(s_0) + v'V^{-1} (Z-\mu(s))$$ $$\sigma^2(s_0) = \sigma^2_0 - v'V^{-1}v$$ with $\mu(s) = (\mu(s_1),\mu(s_2),...,\mu(s_n))'$.

## Unknown, constant mean - Ordinary Kriging

Now suppose the mean is constant, but not known. This is the most simple *realistic* scenario. In this case we include it in the estimation of the weights $\lambda$. In order not to violate the condition of *unbiasedness* we need to make sure that the sum of the weights is 1.

$$\sum_{i=1}^n \lambda_i = 1$$

This can be done by extending the covariance matrix $V$ by one coloumn and one row and the covariance vector $v$ by one element using the *Lagrange* multiplier.


## Unknown, constant mean - Ordinary Kriging


$$\begin{pmatrix}
\lambda_1 \\
\vdots \\
\lambda_n \\
\mathcal{L} \\
\end{pmatrix}  =
\begin{pmatrix}
v_{1,1} & \cdots & v_{1,n} & 1 \\
\vdots & \ddots & \vdots & 1 \\
v_{n,1} & \cdots & v_{n,n} & 1 \\
1 & 1 & 1 & 0 \\
\end{pmatrix}^{-1}
\begin{pmatrix}
v_{1,0} \\
\vdots \\
v_{n,0} \\
1 \\
\end{pmatrix}$$
      
* the Lagrange multiplier ensures that $\sum_{i=1}^n \lambda_i = 1$
* the estimated value calculated by the scalar product $(\lambda_1,...,\lambda_n)  (z_1,...,z_n)'$ 
      
      
## Summary

[**Simple Kriging:**]{.green}

-   assume the mean $\mu$ to be known and constant
-   model local variation $e(s_0)$ as a weighted linear combination of the local variation at the known points $\lambda (Z-\mu)$, then add the mean: $\hat{Z}(s_0) = \mu + \lambda (Z-\mu)$ <br> with $\lambda = v' V^{-1}$

[**Ordinary Kriging:**]{.green}

-   assume the global mean $\mu$ to be constant but *unknown*
-   model global mean $\mu$ and local variation $\lambda (Z-\mu)$ at the same time by estimating the absolute value $\hat{Z}(s_0)$ and making sure that the weights $\lambda$ sum up to 1 (Lagrange multiplier)

## Summary

We also saw what to do when the mean is *varying* but *known*:

\

This is simple kriging with a vector of known means $\mu(s) = (\mu(s_1),\mu(s_2),...,\mu(s_n))'$.
 
## Next

*What if the mean is unknown and varying?*
