---
title: "Lecture 4"
subtitle: "Optimization: stochastic methods"

include-in-header:
  - text: |
      <style>
      .reveal .slide-logo {
        max-height: unset;
        height: 70px;
      }
      </style>
format: revealjs
editor: visual
---

```{r echo=FALSE}
options("digits")
```



## Optimization - Determinisitc vs. stochastic


The algorithms discussed before have been **deterministic**. Given the same input (data, objective function, initial values if non-linear) algorithms produce the same output:

* Linear least squares (normal equations)
* Non-linear one-dimensional (Golden section search)
* Non-linear least squares (Gauss Newton)


## MCMC - Markov chain Monte Carlo methods


The underlying principle is that of MCMC:

**Monte Carlo algorithms:**

- Include randomization to compute results
- Results are not deterministic, running time is (compare Las Vegas algorithms)
- More samples typically lead to better results
- Example: Approximate $\pi$ by counting the portion of $(U(0,1), U(0,1))$ distributed random points that lie inside the unit circle.



## MCMC - Markov chain Monte Carlo methods

**Markov chains:**

- a one-dimensional sequence of random variables (stochastic process) where the next value depends only on the current value but not on past values 
- $P(x_i | x_{i-1},  x_{i-2},  x_{i-3}, ...) = P(x_i | x_{i-1})$
- are autocorrelated
- Examples: Random walk, AR(1)

```{r, error=FALSE,echo=FALSE,warning=FALSE,fig.align='center',fig.width=5, fig.height=2}
library(png)
library(grid)
par(mar=c(0,0,0,0))
img <- readPNG("./data/lec04_markovchain.png")
grid.raster(img)
```



## MCMC - Markov chain Monte Carlo methods

**Idea of MCMC for optimization:**

- Generate a Markov chain by randomly proposing new parameter values based on the current values
- Compute how likely proposed parameters are compared to current values
- Based on that, decide on accepting the proposal value or not

In the end, this chain converges to the stationary distribution of the parameters.


## Metropolis-Hastings
Why would one want probabilistic search?

* global--unlikely areas are searched too (with small probability)
* a probability distribution is richer than a point estimate:

Gauss-Newton provides an estimate of $\hat\theta$ of $\theta$, given
data $y$. What about the estimation error $\hat\theta - \theta$?
Second-order derivatives give approximations to standard errors, but
not the full distribution.

We explain the simplified version, the Metropolis algorithm

## Metropolis algorithm
Given a point in parameter space $\theta$, say $x_t =
(\theta_{1,t},...,\theta_{p,t})$ we evaluate whether another point, $x'$
is a reasonable alternative. If accepted, we set $x_{t+1}\leftarrow x'$; if not
we keep $x_t$ and set $x_{t+1}\leftarrow x_t$.

* if $P(x') > P(x_t)$, we accept $x'$ and set $x_{t+1}=x'$
* if $P(x') < P(x_t)$, then
    * we draw $U$, a random uniform value from $[0,1]$, and
    * accept $x'$ if $U < \frac{P(x')}{P(x_t)}$

Often, $x'$ is drawn from some normal distribution centered around $x_t$:
$N(x_t,\sigma^2 I)$. Suppose we accept it always, then
$$x_{t+1}=x_t + e_t$$
with $e_t \sim N(0,\sigma^2 I)$. Looks familiar?

## Metropolis algorithm


Two steps in the algorithm involve randomness:

- The proposal density  (distribution of jumps), which is often $\theta' \sim \mathcal{N}(\theta_t, \sigma I)$, where $\sigma$ is an important tuning parameter

- The acceptance of worse proposals which depends on the likelihood function of the parameters (see later slides)


## Little mixing (too low acceptance rate):


```{r, echo=FALSE, cache=1, cache.lazy=FALSE, fig.width=6}
Metropolis <- function(theta0, sigma, y, fn, n = 100, debug=FALSE) {
  m = length(theta0)
  out = matrix(NA, n, m)
  out[1,] = theta0 # starting value for the chain
  lastRSS = sum((y - fn(theta0))^2)
  accept = 0
  for (i in 2:n) {
  proposal = rnorm(m, out[i-1,], sigma)
  residualSumSquaresProp = sum((y - fn(proposal))^2)
  s2 = lastRSS/length(y)
  # ratio = exp(-residualSumSquaresProp)/exp(-residualSumSquares0)
  # THIS IS numerically unstable: therefore use
  ratio = exp(-residualSumSquaresProp/(2*s2) + lastRSS/(2*s2))
  if (ratio > 1 || runif(1) < ratio) { # accept
  out[i,] = proposal
  accept = accept + 1
  if (ratio > 1)
  lastRSS = residualSumSquaresProp
  } else
  out[i,] = out[i-1,]
  if (debug && (i %% 500 == 0))
  cat(paste("s2:", s2, "Prop:",
  residualSumSquaresProp/length(y)), "\n")
  }
  #cat("acceptance rate: ", accept/(n-1), "\n")
  class(out) = c("Metropolis", "matrix")
  return(list(chain=out,accept=accept/(n-1)))
}
load("./data/meteo.RData")
f = function(x){x[1]+x[2]*sin(pi*(hours+x[3])/12)}
temp = meteo$T.outside
hours = meteo$hours

set.seed(1)
out = Metropolis(c(18,-4,1.6), c(0.2,0.2,0.2), temp, f, n = 5000)
par(mfcol=c(3,1))
par(mar=c(2, 1, 2, 1))
par(oma=c(3,1,1,0))
par(xpd=NA)
plot(out$chain[3000:5000,1], type="l", ylab="a", xlab="",main=paste("Acceptance rate:", round(out$accept,digits = 4)))
plot(out$chain[3000:5000,2], type="l", ylab="b", xlab="",main="")
plot(out$chain[3000:5000,3], type="l", ylab="c", xlab="Iteration",main="")
```

## Better mixing:


```{r, echo=FALSE, cache=1, fig.width=6}
set.seed(1)
out = Metropolis(c(18,-4,1.6), c(0.08,0.08,0.08), temp, f, n = 5000)
par(mfcol=c(3,1))
par(mar=c(2, 1, 2, 1))
par(oma=c(3,1,1,0))
par(xpd=NA)
plot(out$chain[3000:5000,1], type="l", ylab="a", xlab="",main=paste("Acceptance rate:", round(out$accept,digits = 4)))
plot(out$chain[3000:5000,2], type="l", ylab="b", xlab="",main="")
plot(out$chain[3000:5000,3], type="l", ylab="c", xlab="Iteration",main="")
```



## Still better mixing:


```{r, echo=FALSE, cache=1, fig.width=6}
set.seed(1)
out = Metropolis(c(18,-4,1.6), c(0.03,0.03,0.03), temp, f, n = 5000)
par(mfcol=c(3,1))
par(mar=c(2, 1, 2, 1))
par(oma=c(3,1,1,0))
par(xpd=NA)
plot(out$chain[3000:5000,1], type="l", ylab="a", xlab="",main=paste("Acceptance rate:", round(out$accept,digits = 4)))
plot(out$chain[3000:5000,2], type="l", ylab="b", xlab="",main="")
plot(out$chain[3000:5000,3], type="l", ylab="c", xlab="Iteration",main="")
```


## Little mixing (too high acceptance rate: too much autocorrelation)

```{r, echo=FALSE, cache=1, fig.width=6}
set.seed(1)
out = Metropolis(c(18,-4,1.6), c(0.003,0.003,0.003), temp, f, n = 5000)
par(mfcol=c(3,1))
par(mar=c(2, 1, 2, 1))
par(oma=c(3,1,1,0))
par(xpd=NA)
plot(out$chain[3000:5000,1], type="l", ylab="a", xlab="",main=paste("Acceptance rate:", round(out$accept,digits = 4)))
plot(out$chain[3000:5000,2], type="l", ylab="b", xlab="",main="")
plot(out$chain[3000:5000,3], type="l", ylab="c", xlab="Iteration",main="")
```




## Burn-in 


In the examples above we already omitted the burn-in phase (e.g. the first 3000 samples). If we start with poor parameter values, the complete series might look like:

```{r, echo=FALSE, cache=F, fig.width=6, message=F}
set.seed(1)
out = Metropolis(c(16,-1,3), c(0.03,0.03,0.03), temp, f, n = 5000)
par(mfcol=c(3,1))
par(mar=c(2, 1, 2, 1))
par(oma=c(3,1,1,0))
par(xpd=NA)
plot(out$chain[0:5000,1], type="l", ylab="a", xlab="",main=paste("Acceptance rate:", round(out$accept,digits = 4)))
abline(v=500, xpd=F, col="red", lty="dashed")
plot(out$chain[0:5000,2], type="l", ylab="b", xlab="",main="")
abline(v=500, xpd=F, col="red", lty="dashed")
plot(out$chain[0:5000,3], type="l", ylab="c", xlab="Iteration",main="")
abline(v=500, xpd=F, col="red", lty="dashed")
```



## Burn-in, tuning $\sigma^2$

* When run for a long time, the Metropolis (and its generalization Metropolis-Hastings) 
algorithm provide a _correlated_ sample of the parameter distribution
* M and MH algorithms provide Markov Chain Monte Carlo samples; another
even more popular algorithm is the Gibb's sampler (WinBUGS).
* As the starting value may be quite unlikely, the first part of the 
chain (burn-in) is usually discarded.
* if $\sigma^2$ is too small, the chain mixes too slowly (consecutive
samples are too similar, and do not describe the full PDF)
* if $\sigma^2$ is too large, most proposal values are not accepted
* often, during burn-in, $\sigma^2$ is tuned such that acceptance rate
is close to 60\%.
* many chains can be run, using different starting values, in parallel


## The sample distribution 

We can interpret the results as a sample of the parameter's probability density and use results to compute point estimates and to quantify uncertainties in parameter estimation. 


```{r, echo=FALSE, cache=1, fig.width=6, fig.height=2.5}
set.seed(1)
out = Metropolis(c(16,-1,3), c(0.03,0.03,0.03), temp, f, n = 5000)
par(mfcol=c(1,3))
par(mar=c(4, 3, 1, 2))
#par(oma=c(3,1,1,1));
par(xpd=F)

hist(out$chain[1000:5000,1], main="", xlab="a", freq=F)
hist(out$chain[1000:5000,2], main="", xlab="b", freq=F)
hist(out$chain[1000:5000,3], main="", xlab="c", freq=F)
```



## How do we derive $P(\theta_t)$ and $P(\theta')$

How do we decide whether $\theta'$ is a reasonable alternative?

Idea: We take its _likelihood_ i.e. the probability of the data given the parameters.

$$
P(y_i | \theta) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{\left( y_i - f_{\theta}(x_i) \right)^2}{2 \sigma^2}}
$$

where $y_i - f_{\theta}(x_i) = 0$ has the largest probability, i.e. the maximum likelihood for $\theta$.  


## How do we derive $P(\theta_t)$ and $P(\theta')$

With the independence we can build the likelihood function for $\theta$ given all measurements:



$$
\begin{aligned}
L(\theta) = P(y | \theta) &= \prod_{i=1}^{n}\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{\left( y_i - f_{\theta}(x_i) \right)^2}{2 \sigma^2}} \\
&= \left(\frac{1}{\sigma \sqrt{2 \pi}} \right)^n \cdot \prod_{i=1}^{n} e^{-\frac{\left( y_i - f_{\theta}(x_i) \right)^2}{2 \sigma^2}} \\
&= \left(\frac{1}{\sigma \sqrt{2 \pi}} \right)^n  \cdot e^{-\frac{ \sum_{i=1}^{n} \left( y_i - f_{\theta}(x_i) \right)^2 }{2 \sigma^2}} 
\end{aligned}
$$



## How do we derive $P(\theta_t)$ and $P(\theta')$

Now we can plug in the sum of quared errors

$$
P(y | \theta) = \left(\frac{1}{\sigma \sqrt{2 \pi}} \right)^n e^{-\frac{ \textrm{SSE}_{\theta} }{2 \sigma^2}} 
$$

and can estimate $\sigma^2$ from the data:

$$
\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (y_i - f(x_i))^2 = \frac{\textrm{SSE}_{\theta}}{n} 
$$

(see implementation of the Metropolis algorithm in the lab)


## How do we derive $P(\theta_t)$ and $P(\theta')$

In Metropolis iterations, we need the ratio $\frac{P(\theta')}{P(\theta_t)}$. This reduces to:

$$
\begin{aligned}
\frac{P(\theta')}{P(\theta_t)} &= \frac{\left(\frac{1}{\sigma \sqrt{2 \pi}}\right)^n \cdot e^{-\frac{ \textrm{SSE}' }{2 \sigma^2}}}{\left(\frac{1}{\sigma \sqrt{2 \pi}}\right)^n \cdot e^{-\frac{ \textrm{SSE}_t }{2 \sigma^2}}} \\  
&= \frac{e^{-\frac{ \textrm{SSE}' }{2 \sigma^2}}}{e^{-\frac{ \textrm{SSE}_t' }{2 \sigma^2}}} \\
&= e^{-\frac{ \textrm{SSE}' }{2 \sigma^2} + \frac{ \textrm{SSE}_t }{2 \sigma^2} } \\
&= e^{\frac{\textrm{SSE}_t - \textrm{SSE}'  }{2 \sigma^2} }
\end{aligned}
$$




## Metropolis algorithm

**Probability of accepting a random proposal:**

```{r, echo=FALSE, cache=1, cache.lazy=FALSE, fig.width=6}
ssrdif = seq(-5,5,length.out = 1000) # old_ssr - proposal_ssr
# --> ssrdif > 0 if proposal better
# --> ssrdif < 0 if proposal worse
f <- exp(ssrdif)
f[which(f>1)] = 1
plot(ssrdif,f, type="l", main="", ylab = "Acceptance probability", xlab="Standardized SSE differences (SSE_current - SSE_proposal)", ylim=c(0,1) )
```


## Likelihood ratio -- side track

For evaluating acceptance, the ratio $\frac{P(x')}{P(x_t)}$ is needed,
not the individual values.

This means that $P(x')$ and $P(x_t)$ are only needed _up to a
normalizing constant_: if we have values $aP(x')$ and $aP(x_t)$, than
that is sufficient as $a$ cancels out.

This result is _key_ to the reason that MCMC and M-H are
\color{red}the work horse\color{black}\ in Bayesian statistics, where
$P(x')$ is extremely hard to find because it calls for the evaluation of
a very high-dimensional integral (the normalizing constant that makes
sure that $P(\cdot)$ is a probability) but $aP(x')$, the likelihood of
$x$ given data, is much easier to find!

## Simulated annealing

Simulated Annealing is a related global search algorithm, does not sample
the full parameter distribution but searches for the (global) optimimum.

The analogy with _annealing_, the forming of crystals in a slowly
cooling substance, is the following:

The current solution is replaced by a worse "nearby" solution with a
certain probability that depends on the the degree to which the "nearby"
solution is worse, and on the temperature of the cooling process; this
temperature slowly decreases, allowing less and smaller changes.

## Simulated annealing

At the start, temperature is large and search is close to random; when
temperature decreases search is more and more local and downhill. Random,
uphill jumps prevent SA to fall into a local minimum.

A related algorithm (stochastic optimization) is the Genetic Algorithm.

## Optimization methods by example
